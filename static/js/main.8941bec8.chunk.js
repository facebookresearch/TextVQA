(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{229:function(e){e.exports={textvqa:[{name:"Amanpreet Singh",organization:"Facebook AI Research",website:"https://apsdehal.in",img_url:"https://visualqa.org/static/img/aman.jpg"},{name:"Vivek Natarajan",organization:"",website:"",img_url:"https://visualqa.org/static/img/vivek.jpg"},{name:"Meet Shah",organization:"Facebook AI Research",website:"https://meetshah1995.github.io",img_url:"https://visualqa.org/static/img/meet.jpg"},{name:"Tina Jiang",organization:"Facebook AI Research",website:"",img_url:"https://avatars3.githubusercontent.com/u/7904758?s=460&v=4"},{name:"Xinlei Chen",organization:"Facebook AI Research",website:"https://xinlei.xyz",img_url:"https://visualqa.org/static/img/xinlei.jpg"},{name:"Dhruv Batra",organization:"Facebook AI Research, Georgia Tech",website:"https://www.cc.gatech.edu/~dbatra/",img_url:"https://visualqa.org/static/img/dhruv.jpg"},{name:"Devi Parikh",organization:"Facebook AI Research, Georgia Tech",website:"https://www.cc.gatech.edu/~parikh/",img_url:"https://visualqa.org/static/img/devi.jpg"},{name:"Marcus Rohrbach",organization:"Facebook AI Research",website:"https://rohrbach.vision",img_url:"https://visualqa.org/static/img/marcus.jpg"}],textcaps:[{name:"Oleksii Sidorov",organization:"Facebook AI Research",website:"https://www.linkedin.com/in/sidorovoleksii/",img_url:"https://i.imgur.com/wnCNADH.png"},{name:"Ronghang Hu",organization:"UC Berkeley/Facebook AI Research",website:"https://ronghanghu.com",img_url:"https://ronghanghu.com/wp-content/uploads/2014/12/Myself-6-e1447441544365.jpg"},{name:"Marcus Rohrbach",organization:"Facebook AI Research",website:"https://rohrbach.vision",img_url:"https://visualqa.org/static/img/marcus.jpg"},{name:"Amanpreet Singh",organization:"Facebook AI Research",website:"https://apsdehal.in",img_url:"https://visualqa.org/static/img/aman.jpg"}],textocr:[{name:"Amanpreet Singh*",organization:"Facebook AI Research",website:"https://apsdehal.in",img_url:"https://visualqa.org/static/img/aman.jpg"},{name:"Guan Pang*",organization:"Facebook AI Research",website:"https://www.linkedin.com/in/guan-pang-38aaaa29/",img_url:"https://i.imgur.com/pqIVgJp.png"},{name:"Mandy Toh*",organization:"Facebook AI Research",website:"https://www.linkedin.com/in/marn-yee-toh-2024a8126/",img_url:"https://i.imgur.com/8bzSdkb.png"},{name:"Jing Huang",organization:"Facebook AI Research",website:"https://superirabbit.github.io/",img_url:"https://superirabbit.github.io/img/JingHuang2.png"},{name:"Wojciech Galuba",organization:"Facebook AI Research",website:"https://www.linkedin.com/in/galuba/",img_url:"https://i.imgur.com/RTkVpRq.png"},{name:"Tal Hassner",organization:"Facebook AI Research",website:"https://talhassner.github.io/",img_url:"https://talhassner.github.io/home/images/tal2.jpg"}]}},319:function(e,a,t){e.exports=t(564)},324:function(e,a,t){},49:function(e){e.exports={c:"https://49dppbjhw6.execute-api.us-east-1.amazonaws.com/fair",a:"textvqa",b:"https://eeb1badbfc4b4577862659dc261a631c@sentry.io/1409006"}},530:function(e,a,t){},54:function(e){e.exports={textvqa:{2019:[{name:"Amanpreet Singh",organization:"Facebook AI Research",website:"https://apsdehal.in",img_url:"https://visualqa.org/static/img/aman.jpg"},{name:"Meet Shah",organization:"Facebook AI Research",website:"https://meetshah1995.github.io",img_url:"https://visualqa.org/static/img/meet.jpg"},{name:"Vivek Natarajan",organization:"",website:"",img_url:"https://visualqa.org/static/img/vivek.jpg"},{name:"Xinlei Chen",organization:"Facebook AI Research",website:"https://xinlei.xyz",img_url:"https://visualqa.org/static/img/xinlei.jpg"},{name:"Dhruv Batra",organization:"Facebook AI Research, Georgia Tech",website:"https://www.cc.gatech.edu/~dbatra/",img_url:"https://visualqa.org/static/img/dhruv.jpg"},{name:"Devi Parikh",organization:"Facebook AI Research, Georgia Tech",website:"https://www.cc.gatech.edu/~parikh/",img_url:"https://visualqa.org/static/img/devi.jpg"},{name:"Marcus Rohrbach",organization:"Facebook AI Research",website:"https://rohrbach.vision",img_url:"https://visualqa.org/static/img/marcus.jpg"}],2020:[{name:"Amanpreet Singh",organization:"Facebook AI Research",website:"https://apsdehal.in",img_url:"https://visualqa.org/static/img/aman.jpg"},{name:"Ronghang Hu",organization:"UC Berkeley/Facebook AI Research",website:"https://ronghanghu.com",img_url:"https://ronghanghu.com/wp-content/uploads/2014/12/Myself-6-e1447441544365.jpg"},{name:"Devi Parikh",organization:"Facebook AI Research, Georgia Tech",website:"https://www.cc.gatech.edu/~parikh/",img_url:"https://visualqa.org/static/img/devi.jpg"},{name:"Marcus Rohrbach",organization:"Facebook AI Research",website:"https://rohrbach.vision",img_url:"https://visualqa.org/static/img/marcus.jpg"}],2021:[{name:"Yash Kant",organization:"Georgia Tech",website:"https://yashkant.github.io/",img_url:"https://yashkant.github.io/data/yashkant.jpg"},{name:"Amanpreet Singh",organization:"Facebook AI Research",website:"https://apsdehal.in",img_url:"https://visualqa.org/static/img/aman.jpg"},{name:"Ronghang Hu",organization:"UC Berkeley/Facebook AI Research",website:"https://ronghanghu.com",img_url:"https://ronghanghu.com/wp-content/uploads/2014/12/Myself-6-e1447441544365.jpg"},{name:"Devi Parikh",organization:"Facebook AI Research, Georgia Tech",website:"https://www.cc.gatech.edu/~parikh/",img_url:"https://visualqa.org/static/img/devi.jpg"},{name:"Marcus Rohrbach",organization:"Facebook AI Research",website:"https://rohrbach.vision",img_url:"https://visualqa.org/static/img/marcus.jpg"}]},textcaps:{2020:[{name:"Oleksii Sidorov",organization:"Facebook AI Research",website:"https://www.linkedin.com/in/sidorovoleksii/",img_url:"https://i.imgur.com/wnCNADH.png"},{name:"Ronghang Hu",organization:"UC Berkeley/Facebook AI Research",website:"https://ronghanghu.com",img_url:"https://ronghanghu.com/wp-content/uploads/2014/12/Myself-6-e1447441544365.jpg"},{name:"Devi Parikh",organization:"Facebook AI Research, Georgia Tech",website:"https://www.cc.gatech.edu/~parikh/",img_url:"https://visualqa.org/static/img/devi.jpg"},{name:"Marcus Rohrbach",organization:"Facebook AI Research",website:"https://rohrbach.vision",img_url:"https://visualqa.org/static/img/marcus.jpg"},{name:"Amanpreet Singh",organization:"Facebook AI Research",website:"https://apsdehal.in",img_url:"https://visualqa.org/static/img/aman.jpg"}],2021:[{name:"Yash Kant",organization:"Georgia Tech",website:"https://yashkant.github.io/",img_url:"https://yashkant.github.io/data/yashkant.jpg"},{name:"Oleksii Sidorov",organization:"",website:"https://www.linkedin.com/in/sidorovoleksii/",img_url:"https://i.imgur.com/wnCNADH.png"},{name:"Ronghang Hu",organization:"UC Berkeley/Facebook AI Research",website:"https://ronghanghu.com",img_url:"https://ronghanghu.com/wp-content/uploads/2014/12/Myself-6-e1447441544365.jpg"},{name:"Devi Parikh",organization:"Facebook AI Research, Georgia Tech",website:"https://www.cc.gatech.edu/~parikh/",img_url:"https://visualqa.org/static/img/devi.jpg"},{name:"Marcus Rohrbach",organization:"Facebook AI Research",website:"https://rohrbach.vision",img_url:"https://visualqa.org/static/img/marcus.jpg"},{name:"Amanpreet Singh",organization:"Facebook AI Research",website:"https://apsdehal.in",img_url:"https://visualqa.org/static/img/aman.jpg"}]}}},564:function(e,a,t){"use strict";t.r(a);var n=t(0),l=t.n(n),s=t(45),r=t.n(s),i=t(230),c=(t(324),t(28)),o=t(29),m=t(31),u=t(30),h=t(32),g=t(24),d=t(41),p=t(57),E=t(50),b=t(112),f=t.n(b),v=t(113),x=t.n(v),w=t(12),y=t.n(w),N=t(36),k=t.n(N),C=t(69),T=t.n(C),I=t(83),_=t.n(I),R=t(93),O=t.n(R),A=t(162),S=t.n(A),q=t(26),j=t.n(q),L=t(156),M=t.n(L),V=t(158),P=t.n(V),D=t(160),F=t.n(D),Q=t(219),B=t.n(Q),z=t(159),W=t.n(z),G=t(161),H=t.n(G),U=t(95),J=t.n(U),Y=t(3),K=t.n(Y),X=["textocr","textcaps","textvqa"],Z=function(e){var a;a=e?e.location.pathname:window.location.href;var t=!0,n=!1,l=void 0;try{for(var s,r=X[Symbol.iterator]();!(t=(s=r.next()).done);t=!0){var i=s.value;if(-1!==a.indexOf(i))return i}}catch(c){n=!0,l=c}finally{try{t||null==r.return||r.return()}finally{if(n)throw l}}return"textvqa"},$=Object(g.createMuiTheme)({palette:{primary:{light:"#9ccc65",main:"#7cb342",dark:"#33691e",contrastText:"#fff"},secondary:{light:"#ff7961",main:"#f44336",dark:"#ba000d",contrastText:"#000"}},typography:{useNextVariants:!0}}),ee=Object(g.createMuiTheme)({palette:{primary:{light:"#e34257",main:"#b13444",dark:"#9c2d3c",contrastText:"#fff"},secondary:{light:"#ff7961",main:"#f44336",dark:"#ba000d",contrastText:"#000"}},typography:{useNextVariants:!0}}),ae={textvqa:$,textocr:Object(g.createMuiTheme)({typography:{useNextVariants:!0}}),textcaps:ee}[Z()],te=t(77),ne=t.n(te),le=t(80),se=t.n(le),re=t(79),ie=t.n(re),ce=t(78),oe=t.n(ce),me=t(81),ue=t.n(me),he=ue()()(Object(g.withStyles)(function(e){return{ulList:{margin:"0","& li":{padding:"0.25em"}}}})(function(e){var a=e.fullScreen;return l.a.createElement("div",null,l.a.createElement(ne.a,{fullScreen:a,open:e.open,onClose:e.handleClose,"aria-labelledby":"alert-dialog-title","aria-describedby":"alert-dialog-description"},l.a.createElement(oe.a,{id:"alert-dialog-title"},"Using Explore page"),l.a.createElement(ie.a,{id:"alert-dialog-description"},l.a.createElement("ul",{className:e.classes.ulList},l.a.createElement("li",null,"Bounding boxes show the OCR tokens extracted using the Rosetta OCR system."),l.a.createElement("li",null,"You can click on an example to see a more detailed view for it."),l.a.createElement("li",null,"OCR boxes on some images might be misaligned as they are rotated in OpenImages."),l.a.createElement("li",null,"Some images might not be available on Flickr (404) but should be available through ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/web/download.html"},"OpenImages"),"."),l.a.createElement("li",null,'The text-fields and dropdowns can be mixed to get results that satisfy each of them. You can use this functionality to do an "AND" over multiple fields.'),l.a.createElement("li",null,"Use the ",l.a.createElement("b",null,"'Search in the questions'")," field to search for a phrase in the questions. Press ",l.a.createElement("b",null,"'Enter'")," to get new search results."),l.a.createElement("li",null,"Use the ",l.a.createElement("b",null,"'Choose set'")," dropdown to select the set/s (train and/or val) over which you want to limit your search."),l.a.createElement("li",null,"Use the ",l.a.createElement("b",null,"'Choose classes'")," dropdown to select one or multiple classes over which you want to limit your search."),l.a.createElement("li",null,"Select the checkboxes in ",l.a.createElement("b",null,"'Options'")," to change settings of your search: (i) Exclude OCR boxes (ii) Show questions (iii) Show answers."),l.a.createElement("li",null,"Use the ",l.a.createElement("b",null,"'Search for OCR tokens'")," field to limit your search to particular OCR tokens. This field will also provide auto-complete suggestions."),l.a.createElement("li",null,"Similar to the ",l.a.createElement("b",null,"'Search for OCR tokens'")," field, use the ",l.a.createElement("b",null,"'Search for Answers'")," field to limit your search to particular answers. This field will also provide auto-complete suggestions.")),l.a.createElement("br",null),l.a.createElement("br",null),"Reach us out at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com")," for any questions, suggestions and feedback."),l.a.createElement(se.a,null,l.a.createElement(y.a,{onClick:e.handleClose,color:"primary",autoFocus:!0},"Got it!"))))})),ge=t(5),de=2021,pe={root:{width:"100%",flexGrow:1},grow:{flexGrow:1},buttonLink:{color:ae.palette.primary.contrastText,textDecoration:"none"},buttonsSide:{textAlign:"right"},logo:Object(E.a)({height:"3.6em",verticalAlign:"middle",marginBottom:"-1.0em",marginLeft:"-0.2em"},ae.breakpoints.down("sm"),{height:"2.5em"}),logoText:Object(E.a)({fontSize:"1.0em",fontWeight:"bold",paddingTop:"1.4em",lineHeight:"2",marginLeft:"-0.2em",paddingRight:"0.5em",verticalAlign:"sub"},ae.breakpoints.down("sm"),{fontSize:"1em"}),leftIcon:{marginRight:ae.spacing.unit},sectionDesktop:Object(E.a)({display:"none"},ae.breakpoints.up("md"),{display:"flex"}),sectionMobile:Object(E.a)({display:"flex"},ae.breakpoints.up("md"),{display:"none"}),activeLink:{borderBottom:ae.palette.primary.contrastText+" dotted 3px",marginTop:"-0.2em"},inactiveLink:{borderBottom:"transparent dotted 2px",marginTop:"-0.2em"},otherDatasetIcon:Object(E.a)({height:"3em",marginBottom:"-1.5em",marginTop:"-1em",verticalAlign:"middle"},ae.breakpoints.down("sm"),{marginLeft:"-0.75em"})},Ee={urlPrefix:{textvqa:"/",textcaps:"/textcaps",textocr:"/textocr"},fullText:{textvqa:"TextVQA",textcaps:"TextCaps",textocr:"TextOCR"},logoOnlyWhiteUrl:{textvqa:"/assets/images/textvqa_logo_white.svg",textcaps:"/assets/images/textcaps_logo_white.svg",textocr:"/assets/images/textocr/logo_only_white.svg"},logoOnlyUrl:{textvqa:"/assets/images/textvqa_logo.svg",textcaps:"/assets/images/textcaps_logo.svg",textocr:"/assets/images/textocr/logo_only.svg"},uniqueKey:{textvqa:0,textcaps:1,textocr:2},paperLink:{textvqa:"https://arxiv.org/abs/1904.08920",textcaps:"https://arxiv.org/abs/2003.12462",textocr:"/textocr/paper"},codeLink:{textvqa:"https://github.com/facebookresearch/mmf",textcaps:"https://github.com/facebookresearch/mmf/tree/project/m4c/projects/M4C_Captioner",textocr:"/textocr/code"}},be=["textvqa","textcaps","textocr"],fe=function(e){function a(){var e,t;Object(c.a)(this,a);for(var n=arguments.length,l=new Array(n),s=0;s<n;s++)l[s]=arguments[s];return(t=Object(m.a)(this,(e=Object(u.a)(a)).call.apply(e,[this].concat(l)))).state={mobileMoreAnchorEl:null,challengeAnchorEl:null,dialogOpen:!1},t.handleExploreOpen=function(){t.setState({dialogOpen:!0})},t.handleExploreClose=function(){t.setState({dialogOpen:!1})},t.handleMobileMenuOpen=function(e){t.setState({mobileMoreAnchorEl:e.currentTarget,challengeAnchorEl:e.currentTarget})},t.handleMobileMenuClose=function(e){e.stopPropagation(),t.setState({mobileMoreAnchorEl:null,challengeAnchorEl:null})},t.getYears=function(){var e=2019;-1===t.props.location.pathname.indexOf("textcaps")||(e=2020);for(var a=[],n=e;n<=de;n++)a.push(n);return a.reverse()},t.handleChallengeMenuOpen=function(e){t.setState({challengeAnchorEl:e.currentTarget})},t}return Object(h.a)(a,e),Object(o.a)(a,[{key:"render",value:function(){var e=this,a=this.props.classes,t=this.state,n=t.mobileMoreAnchorEl,s=t.challengeAnchorEl,r=Boolean(n),i=Boolean(s),c=Z(this.props),o="textvqa"===c,m="textocr"===c,u=this.props.location.pathname.split("/"),h=parseInt(-1===u[u.length-1].indexOf("20")?de:u[u.length-1],10),g=this.getYears(),d=l.a.createElement(k.a,{onClick:this.handleMobileMenuClose},l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),onClick:this.handleExploreOpen},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"default"},l.a.createElement(M.a,{className:a.leftIcon}),"Help"))),E=l.a.createElement(J.a,{onClickAway:this.handleMobileMenuClose},l.a.createElement(_.a,null,g.map(function(a){return l.a.createElement(k.a,{component:"a","data-no-link":!0,href:Ee.urlPrefix[c]+"/challenge/"+(h===a?"":a),key:a,width:"100",selected:h===a,onClick:e.handleMobileMenuClose},a)}))),b=l.a.createElement(T.a,{anchorEl:s,open:i,id:"challenge-menu",onClose:this.handleMobileMenuClose,MenuListProps:{onMouseLeave:this.handleMobileMenuClose},getContentAnchorEl:null,anchorOrigin:{vertical:"bottom",horizontal:"right"},transformOrigin:{vertical:"top",horizontal:"right"}},E),v=l.a.createElement(ge.a,{in:!0},l.a.createElement(ge.b,null,g.map(function(e){return l.a.createElement(ge.c,{component:"a","data-no-link":!0,href:Ee.urlPrefix[c]+"/challenge/"+(h===e?"":e),key:e,className:a.buttonsSide,width:"100"},l.a.createElement(y.a,{width:"100",align:"right",disableRipple:!0,disableFocusRipple:!0,color:"default"},l.a.createElement(ge.e,{width:"100",align:"right"},e)))}))),w=l.a.createElement("div",null,l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),onClick:this.handleExploreOpen},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"inherit"},l.a.createElement(M.a,{className:a.leftIcon}),"Help")),l.a.createElement(he,{open:this.state.dialogOpen,handleClose:this.handleExploreClose})),N=[a.buttonLink,a.grow,a.inactiveLink].join(" "),C=be.map(function(e){return e!==c?l.a.createElement(K.a,{underline:"none",key:Ee.uniqueKey[e],className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.urlPrefix[e]},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"inherit"},l.a.createElement("img",{className:a.otherDatasetIcon,srcSet:Ee.logoOnlyWhiteUrl[e],alt:Ee.fullText[e]}),Ee.fullText[e])):""}),I=be.map(function(t){return t!==c?l.a.createElement(k.a,{onClick:e.handleMobileMenuClose},l.a.createElement(K.a,{underline:"none",key:Ee.uniqueKey[t],className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.urlPrefix[t]},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"default"},l.a.createElement("img",{className:a.otherDatasetIcon,srcSet:Ee.logoOnlyWhiteUrl[t],alt:Ee.fullText[t]}),Ee.fullText[t]))):""}),R=l.a.createElement(K.a,{key:Ee.uniqueKey[c],underline:"none",align:"left",className:N,href:"".replace(/\/$/,"")+Ee.urlPrefix[c]},l.a.createElement("img",{className:a.logo,srcSet:Ee.logoOnlyWhiteUrl[c],alt:Ee.fullText[c]}),l.a.createElement("span",{className:a.logoText},Ee.fullText[c])),A=l.a.createElement(T.a,{anchorEl:n,anchorOrigin:{vertical:"top",horizontal:"right"},transformOrigin:{vertical:"top",horizontal:"right"},open:r,onClose:this.handleMobileMenuClose},l.a.createElement(J.a,{onClickAway:this.handleMobileMenuClose},l.a.createElement(_.a,null,l.a.createElement(p.a,{exact:!0,path:"/type:?/explore",render:function(){return d}}),I,l.a.createElement(k.a,{onClick:this.handleMobileMenuClose},l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.urlPrefix[c]+"/dataset"},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"default"},l.a.createElement(j.a,{className:a.leftIcon}),"Dataset"))),m?"":l.a.createElement(k.a,{onClick:this.handleMobileMenuClose},l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.urlPrefix[c]+"/challenge"},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"default"},l.a.createElement(P.a,{className:a.leftIcon}),"Challenge",o?l.a.createElement(W.a,{fontSize:"small"}):""))),m?"":v,l.a.createElement(k.a,{onClick:this.handleMobileMenuClose},l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.paperLink[c]},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"default"},l.a.createElement(F.a,{className:a.leftIcon}),"Paper"))),l.a.createElement(k.a,{onClick:this.handleMobileMenuClose},l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.codeLink[c]},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"default"},l.a.createElement(H.a,{className:a.leftIcon}),"Code"))),l.a.createElement(k.a,{onClick:this.handleMobileMenuClose},l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.urlPrefix[c]+"/explore"},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"default"},l.a.createElement(S.a,{className:a.leftIcon}),"Explore"))))));return l.a.createElement("div",{className:a.root},l.a.createElement(f.a,{position:"static",color:"primary"},l.a.createElement(x.a,null,R,l.a.createElement("div",{className:a.sectionDesktop},l.a.createElement(p.a,{exact:!0,path:"/type?/explore",render:function(){return w}}),C,m?"":l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.urlPrefix[c]+"/challenge"},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,"aria-owns":s||void 0,"aria-haspopup":"true",onClick:this.handleChallengeMenuOpen,onMouseEnter:this.handleChallengeMenuOpen,className:[a.buttonLink,a.buttonsSide].join(" ")},l.a.createElement(P.a,{className:a.leftIcon}),"Challenge",l.a.createElement(W.a,{fontSize:"small"}),b)),l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.paperLink[c]},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"inherit"},l.a.createElement(F.a,{className:a.leftIcon}),"Paper")),l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.codeLink[c]},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"inherit"},l.a.createElement(H.a,{className:a.leftIcon}),"Code")),l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.urlPrefix[c]+"/dataset"},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"inherit"},l.a.createElement(j.a,{className:a.leftIcon}),"Dataset")),l.a.createElement(K.a,{underline:"none",className:[a.buttonLink,a.buttonsSide].join(" "),href:Ee.urlPrefix[c]+"/explore"},l.a.createElement(y.a,{disableRipple:!0,disableFocusRipple:!0,color:"inherit"},l.a.createElement(S.a,{className:a.leftIcon}),"Explore"))),l.a.createElement("div",{className:a.sectionMobile},l.a.createElement(O.a,{"aria-haspopup":"true",onClick:this.handleMobileMenuOpen,color:"inherit"},l.a.createElement(B.a,null))),A)))}}]),a}(l.a.Component),ve=Object(g.withStyles)(pe)(Object(p.f)(fe)),xe=t(85),we=t.n(xe),ye=t(84),Ne=t.n(ye),ke=t(39),Ce=t(2),Te=t.n(Ce),Ie=t(49),_e=t(96),Re=t(114),Oe=t.n(Re),Ae=t(115),Se=t.n(Ae),qe=t(1),je=t.n(qe),Le=t(82),Me=t.n(Le),Ve=function(e){function a(){var e,t;Object(c.a)(this,a);for(var n=arguments.length,l=new Array(n),s=0;s<n;s++)l[s]=arguments[s];return(t=Object(m.a)(this,(e=Object(u.a)(a)).call.apply(e,[this].concat(l)))).colors=["#e6194B","#3cb44b","#ffe119","#f032e6","#9A6324","#800000","#42d4f4","#911eb4"],t.setRotatedCoords=function(e,a,t,n,l,s){var r=e[0],i=e[1],c=r*a+n,o=i*t+l,m=e[2]*a,u=e[3]*t;270===s?(c=(1-i)*a+n,o=r*t+l):180===s?(c=(1-r)*a+n,o=(1-i)*t+l):90===s&&(c=i*a+n,o=(1-r)*t+l),e[0]=c,e[1]=o,e[2]=m,e[3]=u},t.componentDidMount=function(){t.renderCanvas()},t.componentDidUpdate=function(e){e.showBoxes===t.props.showBoxes&&e.imageUrl===t.props.imageUrl||t.renderCanvas()},t.renderCanvas=function(){var e=t.refs.canvas;t.fitToContainer(e);var a=e.getContext("2d"),n=t.props.isDialog,l=new Image;l.src=t.props.imageUrl;var s=parseInt(t.props.rotation,10);0===t.props.rotation.length&&(s=0),l.onload=function(){var e=t.drawImageScaled(l,a);if(a.globalAlpha=1,t.props.showBoxes){var r=e.centerShiftX,i=e.centerShiftY,c=e.newWidth,o=e.newHeight;for(var m in a.font="12px Roboto bold",n&&(a.font="48px Roboto bold"),a.fontWeight="800",t.props.boxes){var u=JSON.parse(JSON.stringify(t.props.boxes[m]));a.beginPath(),a.fillStyle=t.getColor(u.word),a.strokeStyle=a.fillStyle,a.lineWidth=2,t.setRotatedCoords(u.coords,c,o,r,i,s);var h=parseInt(u.rotation,10);a.rotate(h*Math.pi/180),a.strokeRect.apply(a,Object(_e.a)(u.coords)),a.rotate(h*Math.pi/180),n?(a.lineWidth=.5,a.strokeRect(u.coords[0],u.coords[1]-48,a.measureText(u.word).width+15,48),a.fillStyle=t.getColor(u.word),a.globalAlpha=.5,a.rect(u.coords[0],u.coords[1]-48,a.measureText(u.word).width+15,48),a.fill(),a.fillStyle="#000",a.globalAlpha=1,a.fillText(u.word,u.coords[0]+6,u.coords[1]-2)):(a.lineWidth=.5,a.strokeRect(u.coords[0],u.coords[1]-10,a.measureText(u.word).width+15,10),a.fillStyle=t.getColor(u.word),a.globalAlpha=.5,a.rect(u.coords[0],u.coords[1]-10,a.measureText(u.word).width+15,10),a.fill(),a.fillStyle="#000",a.globalAlpha=1,a.fillText(u.word,u.coords[0],u.coords[1])),a.closePath()}}}},t.drawImageScaled=function(e,a){var n=a.canvas,l=n.width/e.width,s=l*e.height;n.style.height=s,n.height=s;var r=n.height/e.height,i=Math.min(l,r),c=(n.width-e.width*i)/2,o=(n.height-e.height*i)/2;return a.clearRect(0,0,n.width,n.height),a.drawImage(e,0,0,e.width,e.height,c,o,e.width*i,e.height*i),t.props.showBoxes&&(a.fillStyle="rgba(0, 0, 0, 0.3)",a.fillRect(c,o,e.width*i,e.height*i)),{centerShiftX:c,centerShiftY:o,newWidth:e.width*i,newHeight:e.height*i}},t}return Object(h.a)(a,e),Object(o.a)(a,[{key:"getColor",value:function(e){var a=e.charCodeAt(0)%this.colors.length;return this.colors[a]}},{key:"fitToContainer",value:function(e){e.style.width="100%",e.width=e.offsetWidth}},{key:"render",value:function(){return l.a.createElement("p",null,l.a.createElement("canvas",{ref:"canvas"}))}}]),a}(n.Component),Pe=t(220),De=t.n(Pe),Fe=ue()()(Object(g.withStyles)(function(e){return{cardContent:{paddingBottom:"4px !important"},card:{padding:2*e.spacing.unit,textAlign:"center"},flickrUrls:Object(E.a)({},e.breakpoints.down("md"),{fontSize:"0.68em"})}})(function(e){return e.result?l.a.createElement("div",null,l.a.createElement(ne.a,{open:e.open,fullWidth:!0,maxWidth:"lg",onClose:e.handleClose,"aria-labelledby":"alert-dialog-title","aria-describedby":"alert-dialog-description"},l.a.createElement(se.a,null,l.a.createElement(je.a,{className:e.classes.flickrUrls,variant:"caption",align:"left"},e.result.flickr_300k_url.length?l.a.createElement(K.a,{target:"_blank",href:e.result.flickr_300k_url},"Flickr Thumbnail"):"",e.result.flickr_original_url.length?l.a.createElement("span",null," | ",l.a.createElement(K.a,{target:"_blank",href:e.result.flickr_original_url},"Original")):""),l.a.createElement(y.a,{size:"small",onClick:e.handleClose,"aria-label":"Close",color:"primary",autoFocus:!0},l.a.createElement(De.a,null))),l.a.createElement(ie.a,{id:"alert-dialog-description"},l.a.createElement(Ve,{showBoxes:e.showOCRBoxes,isDialog:!0,imageUrl:e.result.flickr_original_url,boxes:e.boxes,rotation:e.result.rotation})),l.a.createElement(oe.a,{id:"alert-dialog-title"},l.a.createElement(je.a,{variant:"subtitle1",align:"center"},e.result.question),e.maxAnswer.length>0?l.a.createElement(je.a,{variant:"caption",align:"center"},e.maxAnswer):""))):""})),Qe=function(e){function a(){var e,t;Object(c.a)(this,a);for(var n=arguments.length,s=new Array(n),r=0;r<n;r++)s[r]=arguments[r];return(t=Object(m.a)(this,(e=Object(u.a)(a)).call.apply(e,[this].concat(s)))).seed=Math.round(1e4*Math.random()),t.state={render:0,currentDialog:-1},t.vars={nChunks:null,originalChunks:[],originalIds:[],originalLen:0,currentResult:null,currentMaxAnswer:null,currentBoxes:null},t.handleDialogOpen=function(e){t.vars.currentMaxAnswer=e.currentMaxAnswer,t.vars.currentBoxes=e.currentBoxes,t.vars.currentResult=e.currentResult,t.setState({currentDialog:e.currentDialog})},t.handleDialogClose=function(){t.setState({currentDialog:-1})},t.getWidthForColumn=function(){var e={xs:"100",sm:"50",md:"33",lg:"25"},a="100";return Object.keys(e).forEach(function(n){Object(Le.isWidthUp)(n,t.props.width)&&(a=e[n])}),a},t.chunkArray=function(e,a){var n=!1,l="question_id";"textcaps"===t.props.match.params.type&&(l="image_id");var s=[];if(e.forEach(function(e){s.push(e[l])}),0===e.length?n=!0:t.vars.originalChunks.length>0&&t.vars.originalChunks[0].length>0&&(t.vars.originalChunks[0][0][l]!==e[0][l]||JSON.stringify(t.vars.originalIds)!==JSON.stringify(s.slice(0,t.vars.originalLen)))&&(n=!0),t.vars.nChunks!==a||n){t.vars.nChunks=a,t.vars.originalChunks=[],t.vars.originalLen=0,t.seed=Math.round(1e4*Math.random());for(var r=0;r<a;r++)t.vars.originalChunks.push([])}var i=Math.floor((e.length-t.vars.originalLen)/a),c=0,o=t.vars.originalChunks;0===i&&(i=1);for(var m=t.vars.originalLen;m<e.length;m+=i)c===t.vars.nChunks&&(c=Math.floor(Math.random()*t.vars.nChunks)),o[c]=o[c].concat(e.slice(m,m+i)),c++;return t.vars.originalChunks=o,t.vars.originalLen=e.length,t.vars.nChunks=a,t.vars.originalIds=s,o},t.renderCardComponents=function(e){if(e.error)return l.a.createElement("div",null," Some error happened! ");if(console.log(e),0===e.length)return"";var a=[].concat(Object(_e.a)(e.data),Object(_e.a)(e.streamData)),n=t.getWidthForColumn(),s=Math.floor(100/n),r=t.chunkArray(a,s).map(function(e,a){return e.map(function(e,a){return t.renderCardComponent(e,a)})}).map(function(e,a){return l.a.createElement("div",{key:a+t.seed,style:{width:n+"%",float:"left"}},e)});return l.a.createElement("div",null,r)},t.getMaxAnswer=function(e){var a={};if(!e)return"";var t=e[0],n=1;for(var l in e){var s=e[l];a[s]?a[s]+=1:a[s]=1,a[s]>n&&(n=a[s],t=s)}return t},t.getMainCardContent=function(e){if(e.question)return l.a.createElement(je.a,{variant:"subtitle1"},e.question);var a=[];return e.captions&&e.captions.forEach(function(e,t){a.push(l.a.createElement(je.a,{variant:"subtitle1",align:"left"},t+1+". "+e))}),a},t.renderCardComponent=function(e,a){var n=[];for(var s in e.ocr_info){var r=e.ocr_info[s],i={coords:[r.bounding_box.top_left_x,r.bounding_box.top_left_y,r.bounding_box.width,r.bounding_box.height],rotation:r.bounding_box.rotation,word:r.word};n.push(i)}var c=t.getMaxAnswer(e.answers);return l.a.createElement("div",{key:a+t.seed,className:t.props.classes.gridItem,onClick:function(){return t.handleDialogOpen({currentDialog:a+t.seed,currentBoxes:n,currentResult:e,currentMaxAnswer:c})}},l.a.createElement(Oe.a,{key:a,className:t.props.classes.card},l.a.createElement(Ve,{showBoxes:t.props.showOCRBoxes,imageUrl:e.flickr_300k_url,boxes:n,rotation:e.rotation}),l.a.createElement(Se.a,{className:t.props.classes.cardContent},t.props.showQuestions?t.getMainCardContent(e):"",t.props.showAnswers&&c.length>0?l.a.createElement(je.a,{variant:"caption"},c):"")))},t.formatResultsStats=function(e){return l.a.createElement(je.a,{className:t.props.classes.gridItem,component:"span",align:"center"},e.totalResults," results found in ",e.time,"ms.",l.a.createElement("br",null),'For more information on how to use search, please see "Help" in navigation bar.',l.a.createElement("br",null))},t.noResultStats=function(e){return l.a.createElement(je.a,{className:t.props.classes.gridItem,component:"span",align:"center"},"No results found.",l.a.createElement("br",null),'For more information on how to use search, please see "Help" in navigation bar.',l.a.createElement("br",null))},t}return Object(h.a)(a,e),Object(o.a)(a,[{key:"render",value:function(){var e="question";return"textcaps"===this.props.match.params.type&&(e="captions"),l.a.createElement(Te.a,{container:!0,direction:"row",justify:"center",alignItems:"center"},l.a.createElement(ke.d,{componentId:"result",dataField:e,title:"Results",from:0,size:this.props.size||25,renderResultStats:this.formatResultsStats,pagination:this.props.pagination||!1,showResultStats:this.props.showResultStats,loader:this.props.loader,renderNoResults:this.noResultStats,react:this.props.reactValues,render:this.renderCardComponents,style:this.props.style||{}}),this.props.dialogEnabled?l.a.createElement(Fe,{maxAnswer:this.vars.currentMaxAnswer,result:this.vars.currentResult,showOCRBoxes:this.props.showOCRBoxes,boxes:this.vars.currentBoxes,open:-1!==this.state.currentDialog,handleClose:this.handleDialogClose}):"")}}]),a}(n.Component),Be=Me()()(Object(g.withStyles)(function(e){return{gridItem:{padding:.5*e.spacing.unit,position:"relative",width:"100%",display:"inline-block"},cardContent:{paddingBottom:"4px !important"},card:{padding:2*e.spacing.unit,textAlign:"center"}}})(Object(p.f)(Qe))),ze=function(e){function a(){var e,t;Object(c.a)(this,a);for(var n=arguments.length,l=new Array(n),s=0;s<n;s++)l[s]=arguments[s];return(t=Object(m.a)(this,(e=Object(u.a)(a)).call.apply(e,[this].concat(l)))).seed=Math.round(1e4*Math.random()),t.state={showOCRBoxes:!0,showQuestions:!0,showAnswers:!0,anchorEl:null},t.reactValues={and:["searchbox","set_name","image_classes","ocr_tokens","answers"]},t.replaceBody=function(e){var a=t.props.match.params.type;return e=(e=(e=e.join("\n")).replace('"field":"set_name"','"field":"set_name"')).replace('"field":"image_classes"','"field":"image_classes"'),e=(e="textcaps"===a?e.replace('"field":"captions"','"field":"captions"'):(e=e.replace('"field":"question"','"field":"question"')).replace('"field":"answers"','"field":"answers"')).replace('"field":"ocr_tokens"','"field":"ocr_tokens"')},t.updateQuery=function(e){var a=e.body;a=a.split("\n");var n=JSON.parse(a[1]);return n.query={function_score:{query:n.query,random_score:{seed:t.seed}}},a[1]=JSON.stringify(n),a=t.replaceBody(a),e.body=a,e},t.handleCheckboxChange=function(e){return function(a){t.setState(Object(E.a)({},e,a.target.checked))}},t.handleMenuButtonClick=function(e){t.setState({anchorEl:e.currentTarget})},t.handleMenuClose=function(){t.setState({anchorEl:null})},t.getReactArray=function(e){var a=[],n=(a="textcaps"===t.props.match.params.type?["set_name","image_classes","ocr_tokens","captions"]:["set_name","image_classes","ocr_tokens","question","answer"]).indexOf(e);return-1!==n&&a.splice(n,1),a},t}return Object(h.a)(a,e),Object(o.a)(a,[{key:"render",value:function(){var e=this.props.match.params.type,a="question",t="Search in the questions",n=!0,s="Show Questions";return"textocr"===e?"Explore page will be active soon!":("textcaps"===e&&(a="captions",t="Search in the captions",n=!1,s="Show Captions",Ie.a="textcaps"),l.a.createElement(ke.c,{app:Ie.a,url:Ie.c,transformRequest:this.updateQuery},l.a.createElement(Te.a,{container:!0,direction:"row",className:this.props.classes.root,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:12,md:6,lg:4},l.a.createElement(ke.a,{componentId:"searchbox",dataField:a,autosuggest:!1,categoryField:a,placeholder:t,debounce:1e3,style:{padding:"5px"},react:{and:this.getReactArray(a)}})),l.a.createElement(Te.a,{item:!0,xs:12,md:2,lg:1},l.a.createElement(ke.b,{componentId:"set_name",dataField:"set_name",showCount:!1,placeholder:"Choose set",showSearch:!1,react:{and:this.getReactArray("set_name")}})),l.a.createElement(Te.a,{item:!0,xs:12,md:2,lg:2},l.a.createElement(ke.b,{componentId:"image_classes",dataField:"image_classes",placeholder:"Choose classes",showSearch:!1,react:{and:this.getReactArray("image_classes")}})),l.a.createElement(Te.a,{item:!0,xs:12,md:2,lg:1},l.a.createElement(y.a,{variant:"contained",color:"primary",className:this.props.classes.optionsMenuButton,"aria-owns":this.state.anchorEl?"simple-menu":void 0,"aria-haspopup":"true",onClick:this.handleMenuButtonClick},"Options"),l.a.createElement(T.a,{anchorEl:this.state.anchorEl,open:Boolean(this.state.anchorEl),onClose:this.handleMenuClose},l.a.createElement(k.a,null,l.a.createElement(Ne.a,{control:l.a.createElement(we.a,{checked:this.state.showOCRBoxes,onChange:this.handleCheckboxChange("showOCRBoxes"),value:"showOCRBoxes"}),label:"Show OCR boxes"})),l.a.createElement(k.a,null,l.a.createElement(Ne.a,{control:l.a.createElement(we.a,{checked:this.state.showQuestions,onChange:this.handleCheckboxChange("showQuestions"),value:"showQuestions"}),label:s})),n?l.a.createElement(k.a,null,l.a.createElement(Ne.a,{control:l.a.createElement(we.a,{checked:this.state.showAnswers,onChange:this.handleCheckboxChange("showAnswers"),value:"showAnswers"}),label:"Show answers"})):"")),l.a.createElement(Te.a,{item:!0,xs:12,md:6,lg:2},l.a.createElement(ke.a,{componentId:"ocr_tokens",dataField:"ocr_tokens",categoryField:"ocr_tokens",debounce:1400,placeholder:"Search for OCR tokens",style:{padding:"5px"},react:{and:this.getReactArray("ocr_tokens")}})),n?l.a.createElement(Te.a,{item:!0,xs:12,md:6,lg:2},l.a.createElement(ke.a,{componentId:"answers",dataField:"answers",categoryField:"answers",placeholder:"Search for Answers",debounce:1400,style:{padding:"5px"},react:{and:this.getReactArray("answers")}})):"",l.a.createElement(Be,{showOCRBoxes:this.state.showOCRBoxes,showAnswers:this.state.showAnswers,showQuestions:this.state.showQuestions,reactValues:this.reactValues,showResultStats:!0,pagination:!1,size:25,style:{width:"90%",textAlign:"center"},dialogEnabled:!0}))))}}]),a}(n.Component),We=Object(g.withStyles)(function(e){return{root:{flexGrow:1},optionsMenuButton:{width:"100%"}}})(Object(p.f)(ze)),Ge=t(228),He=t(46),Ue=t.n(He),Je=t(8),Ye=t.n(Je),Ke=t(229),Xe=t(117),Ze=t.n(Xe),$e=Object(g.withStyles)(function(e){return{avatar:{width:100,height:100,margin:"0 auto"},gridItem:{padding:2*e.spacing.unit}}})(function(e){var a=[],t=2,n=Object(p.e)(),s=Z({location:n});a=e.people?e.people:Ke[s],e.lgSize&&(t=e.lgSize);var r=a.map(function(a){var n=Math.random();return l.a.createElement(Te.a,{item:!0,key:n,xs:12,sm:6,md:3,lg:t},l.a.createElement(K.a,{target:"_blank",rel:"noopener",href:a.website},l.a.createElement(Ze.a,{className:e.classes.avatar,src:a.img_url})),l.a.createElement(ge.e,{variant:"subtitle1"},a.name),l.a.createElement(ge.e,{variant:"caption"},a.organization))});return l.a.createElement(Te.a,{container:!0,direction:"row",justify:"center",alignItems:"flex-start",spacing:16},r)}),ea=t(86),aa=function(e){function a(){return Object(c.a)(this,a),Object(m.a)(this,Object(u.a)(a).apply(this,arguments))}return Object(h.a)(a,e),Object(o.a)(a,[{key:"render",value:function(){var e=this.props.classes;return l.a.createElement("div",{className:e.root},l.a.createElement(Te.a,{container:!0,className:e.mainContainer,direction:"row",justify:"center",alignItems:"center",style:{position:"relative"}},l.a.createElement("div",{style:{height:"50vh",backgroundColor:"rgba(0, 0, 0, 0.4)",width:"100%",zIndex:-10,position:"absolute"}}),l.a.createElement(ke.c,{app:Ie.a,url:Ie.c,transformRequest:this.updateQuery,style:{height:"50vh",overflow:"auto",position:"relative"}},l.a.createElement(Be,{showOCRBoxes:!0,showAnswers:!0,showResultStats:!1,showQuestions:!0,reactValues:{},pagination:!0,loader:"",size:15,style:{height:"50vh",zIndex:-20,size:10,position:"absolute",top:0,overflow:"hidden"}})),l.a.createElement(Te.a,{container:!0,style:{position:"absolute",top:0,left:0,width:"100%",height:"100%"},justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,sm:9,md:8,lg:6,style:{position:"relative"}},l.a.createElement(Ue.a,{className:e.paper,elevation:1},l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:6},l.a.createElement("img",{srcSet:"/assets/images/textvqa_logo_and_text_green.svg",alt:"TextVQA",className:e.bannerLogo}))),l.a.createElement(je.a,{variant:"subtitle1"},"A dataset to benchmark visual reasoning based on text in images."),l.a.createElement(Te.a,{style:{marginTop:"10px"},container:!0,direction:"row",justify:"center",alignItems:"center",spacing:16},l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"explore"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Explore"))),l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"https://arxiv.org/abs/1904.08920"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Paper"))),l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"challenge"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Challenge"))),l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"dataset"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Download")))))))),l.a.createElement(Te.a,{container:!0,justify:"center",alignContent:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7,className:e.people},l.a.createElement(Te.a,{container:!0,justify:"center",alignContent:"center"},l.a.createElement(Te.a,{item:!0,xs:12,lg:6},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",gutterBottom:!0,align:"left"},"Overview")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"subtitle1",gutterBottom:!0,align:"left"},"TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions.")))),l.a.createElement(Te.a,{item:!0,xs:12,lg:6},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",gutterBottom:!0,align:"left"},"Statistics"))),l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement("ul",null,l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"28,408 images from OpenImages")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"45,336 questions")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"453,360 ground truth answers"))))))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"News")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},"Join our ",l.a.createElement(K.a,{href:"https://groups.google.com/forum/#!forum/textvqa"},"Google Group")," for TextVQA release updates and announcements.")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement("ul",{className:e.ulList},ea.textvqa.news.map(function(a,t){return l.a.createElement("li",{key:t},l.a.createElement(je.a,{align:"left",variant:"subtitle1"},l.a.createElement("span",null,"[",a.date,"] "),l.a.createElement("span",{className:e.hrefGreenColor,dangerouslySetInnerHTML:{__html:a.news}})))}))))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Challenge")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"TextVQA Challenge 2021 is live! See more details on the ",l.a.createElement(K.a,{href:"challenge"},"challenge page"),".")))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Code")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left",className:e.breakWord},"Find the starter code for TextVQA and LoRRA at ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia"},"https://github.com/facebookresearch/pythia"),".")))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Citation")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement("pre",{className:e.citationPre},l.a.createElement("code",null,"@inproceedings","{","singh2019towards,",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0title=","{","Towards VQA Models That Can Read},",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0author=","{","Singh, Amanpreet and Natarjan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Parikh, Devi and Rohrbach, Marcus},",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0booktitle=","{","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0pages=","{","8317-8326},",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0year=","{","2019}",l.a.createElement("br",null),"}")),"or use this ",l.a.createElement(K.a,{href:"/bibtex.txt"},"link"),"."),l.a.createElement("br",null)))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"People"))),l.a.createElement("div",{className:e.container}),l.a.createElement($e,null)),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Contact")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Reach us out at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com")," for any questions, suggestions and feedback.")))),l.a.createElement(Ye.a,{className:e.people}),l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignContent:"center",spacing:16,className:e.sectionHeader},l.a.createElement(Te.a,{item:!0,xs:6,md:6,lg:4},l.a.createElement(K.a,{target:"_blank",href:"https://research.fb.com/category/facebook-ai-research/"},l.a.createElement("img",{className:e.bannerLogo,style:{marginTop:"5%"},srcSet:"/assets/images/fair_logo.png",alt:"Facebook Artificial Intelligence Research"}))),l.a.createElement(Te.a,{item:!0,xs:3,md:3,lg:2},l.a.createElement(K.a,{target:"_blank",href:"https://www.gatech.edu/"},l.a.createElement("img",{className:e.bannerLogo,srcSet:"/assets/images/gt_logo.png",alt:"Georgia Tech"})))))))}}]),a}(l.a.Component),ta=function(e){function a(){return Object(c.a)(this,a),Object(m.a)(this,Object(u.a)(a).apply(this,arguments))}return Object(h.a)(a,e),Object(o.a)(a,[{key:"render",value:function(){var e=this.props.classes;return l.a.createElement("div",{className:e.root},l.a.createElement(Te.a,{container:!0,className:e.mainContainer,direction:"row",justify:"center",alignItems:"center",style:{position:"relative"}},l.a.createElement("div",{style:{height:"50vh",backgroundColor:"rgba(0, 0, 0, 0.4)",width:"100%",zIndex:-10,position:"absolute"}}),l.a.createElement(ke.c,{app:"textcaps",url:Ie.c,transformRequest:this.updateQuery,style:{height:"50vh",overflow:"auto",position:"relative"}},l.a.createElement(Be,{showOCRBoxes:!0,showAnswers:!0,showResultStats:!1,showQuestions:!0,reactValues:{},pagination:!0,loader:"",size:15,style:{height:"50vh",zIndex:-20,size:10,position:"absolute",top:0,overflow:"hidden"}})),l.a.createElement(Te.a,{container:!0,style:{position:"absolute",top:0,left:0,width:"100%",height:"100%"},justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,sm:9,md:8,lg:6,style:{position:"relative"}},l.a.createElement(Ue.a,{className:e.paper,elevation:1},l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:6},l.a.createElement("img",{srcSet:"/assets/images/textcaps_logo_and_text.svg",alt:"TextCaps",className:e.bannerLogo}))),l.a.createElement(je.a,{variant:"subtitle1"},"A dataset to benchmark reading comprehension in image captioning task."),l.a.createElement(Te.a,{style:{marginTop:"10px"},container:!0,direction:"row",justify:"center",alignItems:"center",spacing:16},l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"/textcaps/explore"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Explore"))),l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"https://arxiv.org/abs/2003.12462"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Paper"))),l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"/textcaps/challenge"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Challenge"))),l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"/textcaps/dataset"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Download")))))))),l.a.createElement(Te.a,{container:!0,justify:"center",alignContent:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7,className:e.people},l.a.createElement(Te.a,{container:!0,justify:"center",alignContent:"center"},l.a.createElement(Te.a,{item:!0,xs:12,lg:6},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",gutterBottom:!0,align:"left"},"Overview")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"subtitle1",gutterBottom:!0,align:"left"},"TextCaps requires models to read and reason about text in images to generate captions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it and visual content in the image to generate image descriptions.")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"subtitle1",gutterBottom:!0,align:"left"},"Have a look at our ECCV 2020 oral presentation to know more about TextCaps:",l.a.createElement("iframe",{title:"ECCV Video",width:"560",height:"315",src:"https://www.youtube.com/embed/bWOnRpqmom4",frameBorder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0}))))),l.a.createElement(Te.a,{item:!0,xs:12,lg:6},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",gutterBottom:!0,align:"left"},"Statistics"))),l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement("ul",null,l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"28,408 images from OpenImages")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"142,040 captions")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"5 captions per image"))))))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"News")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"subtitle1",align:"left"},"Join our ",l.a.createElement(K.a,{href:"https://groups.google.com/forum/#!forum/textvqa"},"Google Group")," for TextCaps release updates and announcements.")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement("ul",{className:e.ulList},ea.textcaps.news.map(function(a,t){return l.a.createElement("li",{key:t},l.a.createElement(je.a,{align:"left",variant:"subtitle1"},l.a.createElement("span",null,"[",a.date,"] "),l.a.createElement("span",{className:e.hrefGreenColor,dangerouslySetInnerHTML:{__html:a.news}})))}))))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Challenge")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"TextCaps Challenge 2020 is live! See more details on the ",l.a.createElement(K.a,{href:"/textcaps/challenge"},"challenge page"),".")))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Code")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left",className:e.breakWord},"Find the starter code for TextCaps and M4C-Captioner at ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/tree/project/m4c/projects/M4C_Captioner"},"https://github.com/facebookresearch/pythia"),".")))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Citation")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement("pre",{className:e.citationPre},l.a.createElement("code",null,"@inproceedings","{","sidorov2019textcaps,",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0title=","{","TextCaps: a Dataset for Image Captioningwith Reading Comprehension},",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0author=","{","Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0journal=","{","European Conference on Computer Vision},",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0year=","{","2020}",l.a.createElement("br",null),"}")),"or use this ",l.a.createElement(K.a,{href:"/textcaps_bibtex.txt"},"link"),"."),l.a.createElement("br",null)))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"People"))),l.a.createElement("div",{className:e.container}),l.a.createElement($e,null)),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Contact")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Reach us out at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com")," for any questions, suggestions and feedback.")))),l.a.createElement(Ye.a,{className:e.people}),l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignContent:"center",spacing:16,className:e.sectionHeader},l.a.createElement(Te.a,{item:!0,xs:6,md:6,lg:4},l.a.createElement(K.a,{target:"_blank",href:"https://research.fb.com/category/facebook-ai-research/"},l.a.createElement("img",{className:e.bannerLogo,style:{marginTop:"5%"},srcSet:"/assets/images/fair_logo.png",alt:"Facebook Artificial Intelligence Research"}))),l.a.createElement(Te.a,{item:!0,xs:3,md:3,lg:2},l.a.createElement(K.a,{target:"_blank",href:"https://www.berkeley.edu/"},l.a.createElement("img",{className:e.bannerLogo,style:{marginTop:"5%"},srcSet:"https://www.berkeley.edu/images/uploads/logo-ucberkeley.png",alt:"UC Berkeley"})))))))}}]),a}(l.a.Component),na=function(e){function a(){return Object(c.a)(this,a),Object(m.a)(this,Object(u.a)(a).apply(this,arguments))}return Object(h.a)(a,e),Object(o.a)(a,[{key:"render",value:function(){var e=this.props.classes;return l.a.createElement("div",{className:e.root},l.a.createElement(Te.a,{container:!0,className:e.mainContainer,direction:"row",justify:"center",alignItems:"center",style:{position:"relative"}},l.a.createElement("div",{style:{height:"50vh",backgroundColor:"rgba(0, 0, 0, 0.4)",width:"100%",zIndex:-10,position:"absolute"}}),l.a.createElement(Te.a,{container:!0,style:{position:"absolute",top:0,left:0,width:"100%",height:"100%"},justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,sm:9,md:8,lg:6,style:{position:"relative"}},l.a.createElement(Ue.a,{className:e.paper,elevation:1},l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:6},l.a.createElement("img",{srcSet:"/assets/images/textocr/logo_horizontal_color_with_text.svg",alt:"TextCaps",className:e.bannerLogo}))),l.a.createElement(je.a,{variant:"subtitle1"},"A dataset to benchmark text recognition on arbitrary shaped scene-text."),l.a.createElement(Te.a,{style:{marginTop:"10px"},container:!0,direction:"row",justify:"center",alignItems:"center",spacing:16},l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"/textocr/explore"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Explore"))),l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"https://arxiv.org/abs/2003.12462"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Paper"))),l.a.createElement(Te.a,{item:!0,xs:6,sm:6,md:3},l.a.createElement(K.a,{href:"/textocr/dataset"},l.a.createElement(y.a,{variant:"contained",color:"primary"},"Download")))))))),l.a.createElement(Te.a,{container:!0,justify:"center",alignContent:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7,className:e.people},l.a.createElement(Te.a,{container:!0,justify:"center",alignContent:"center"},l.a.createElement(Te.a,{item:!0,xs:12,lg:6},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",gutterBottom:!0,align:"left"},"Overview")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"subtitle1",gutterBottom:!0,align:"left"},"TextOCR requires models to perform text-recognition on arbitrary shaped scene-text present on natural images. TextOCR provides ~1M high quality word annotations on TextVQA images allowing application of end-to-end reasoning on downstream tasks such as visual question answering or image captioning.")))),l.a.createElement(Te.a,{item:!0,xs:12,lg:6},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",gutterBottom:!0,align:"left"},"Statistics"))),l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement("ul",null,l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"28,134 natural images from TextVQA")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"903,069 annotated scene-text words")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"32 words per image on average"))))))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"News")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement("ul",{className:e.ulList},ea.textocr.news.map(function(a,t){return l.a.createElement("li",{key:t},l.a.createElement(je.a,{align:"left",variant:"subtitle1"},l.a.createElement("span",null,"[",a.date,"] "),l.a.createElement("span",{className:e.hrefGreenColor,dangerouslySetInnerHTML:{__html:a.news}})))}))))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Challenge")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Coming soon.")))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Code")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left",className:e.breakWord},"Coming soon!")))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Citation")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement("pre",{className:e.citationPre},l.a.createElement("code",null,"@inproceedings","{","singh2021textocr,",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0title=","{","{","TextOCR}: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text},",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0author=","{","Singh, Amanpreet and Pang, Guan and Toh, Mandy and Huang, Jing and Galuba, Wojciech and Hassner, Tal},",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0journal=","{","The Conference on Computer Vision and Pattern Recognition},",l.a.createElement("br",null),"\xa0\xa0\xa0\xa0year=","{","2021}",l.a.createElement("br",null),"}")),"or use this ",l.a.createElement(K.a,{href:"/textcaps_bibtex.txt"},"link"),"."),l.a.createElement("br",null)))),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"People"))),l.a.createElement("div",{className:e.container}),l.a.createElement($e,null),l.a.createElement(je.a,{className:e.sectionHeader,variant:"caption",align:"center"},"*Equal contribution")),l.a.createElement("div",{className:e.people},l.a.createElement(Te.a,{container:!0,justify:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{className:e.sectionHeader,variant:"h4",align:"left"},"Contact")),l.a.createElement(Te.a,{item:!0,xs:12,className:e.gridItem},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Reach us out at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com")," for any questions, suggestions and feedback.")))),l.a.createElement(Ye.a,{className:e.people}),l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignContent:"center",spacing:16,className:e.sectionHeader},l.a.createElement(Te.a,{item:!0,xs:6,md:6,lg:4},l.a.createElement(K.a,{target:"_blank",href:"https://research.fb.com/category/facebook-ai-research/"},l.a.createElement("img",{className:e.bannerLogo,style:{marginTop:"5%"},srcSet:"/assets/images/fair_logo.png",alt:"Facebook Artificial Intelligence Research"})))))))}}]),a}(l.a.Component),la=function(e){return{paper:Object(Ge.a)({},e.mixins.gutters(),{paddingTop:2*e.spacing.unit,paddingBottom:2*e.spacing.unit}),mainContainer:{backgroundImage:"radial-gradient( "+e.palette.primary.light+" 20% ,"+e.palette.primary.dark+" 100%)",opacity:.9,height:"50vh"},people:{margin:"0 auto",marginTop:"1.5em"},root:{},divider:{color:"#888",width:"100%"},gridItem:{padding:1.5*e.spacing.unit},sectionHeader:{marginTop:"0.15em"},ulList:{margin:"0"},container:{padding:2*e.spacing.unit},bannerLogo:{width:"100%"},spanInlineBlock:{display:"inline"},greenColor:{color:e.palette.primary.main},hrefGreenColor:{"& a":{color:e.palette.primary.main,textDecoration:"none"}},preParent:{backgroundColor:"#eee",border:"1px solid #ddd",borderRadius:"2px","& pre":{whiteSpace:"pre-wrap"},"& span":{padding:"1em"},"& code":{fontSize:"12px",color:"#000"}},citationPre:{backgroundColor:"#eee",whiteSpace:"pre-wrap",wordBreak:"break-all",padding:"1em"},breakWord:{wordBreak:"break-all",whiteSpace:"pre-wrap"}}},sa=function(e){function a(){var e,t;Object(c.a)(this,a);for(var n=arguments.length,l=new Array(n),s=0;s<n;s++)l[s]=arguments[s];return(t=Object(m.a)(this,(e=Object(u.a)(a)).call.apply(e,[this].concat(l)))).state={checked:!1},t.updateQuery=function(e){var a=e.body;a=a.split("\n");var n=JSON.parse(a[1]);return n.query={function_score:{query:n.query,random_score:{seed:t.seed}}},a[1]=JSON.stringify(n),a=(a=(a=a.join("\n")).replace('"field":"set_name"','"field":"set_name.keyword"')).replace('"field":"image_classes"','"field":"image_classes.keyword"'),e.body=a,e},t}return Object(h.a)(a,e),Object(o.a)(a,[{key:"render",value:function(){var e=Z(this.props),a=Object(g.withStyles)(la)(aa);return"textcaps"===e?a=Object(g.withStyles)(la)(ta):"textocr"==e&&(a=Object(g.withStyles)(la)(na)),l.a.createElement(a,null)}}]),a}(l.a.Component),ra=Object(g.withStyles)(la)(Object(p.f)(sa)),ia=(t(530),t(53)),ca=t(47),oa=t.n(ca),ma=t(25),ua=t.n(ma),ha=t(6),ga=t.n(ha),da=t(48),pa=t.n(da),Ea=t(18),ba=t.n(Ea),fa=t(54),va=t(55),xa=t.n(va),wa=function(){return l.a.createElement("span",null,"Challenge has completed!")},ya=function(e){var a=e.days,t=e.hours,n=e.minutes,s=e.seconds;return e.completed?l.a.createElement(wa,null):l.a.createElement("span",null,a," days ",t,"h ",n,"m ",s,"s")},Na=Object(g.withStyles)(function(e){return{title:{marginTop:"3em"},overview:{marginTop:"2em"},headings:{marginTop:"1em"},ulItems:{paddingLeft:"1.2em"},preParent:{backgroundColor:"#eee",border:"1px solid #ddd",borderRadius:"2px","& pre":{whiteSpace:"pre-wrap"},"& span":{padding:"1em"},"& code":{fontSize:"12px",color:"#000"}},evalAILink:{wordBreak:"break-word"},teaserImage:{width:"100%"},spanTypography:{display:"inline-block"},bannerLogo:{width:"100%"},sectionHeader:{marginTop:"0.35em"},evalaiLogo:{marginLeft:"-3px"}}})(function(e){var a=xa.a.tz("2019-05-27T23:59:59","Etc/GMT").toDate(),t=-1===Object(p.e)().pathname.indexOf("textcaps")?"textvqa":"textcaps";return l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7},l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignItems:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12},l.a.createElement(je.a,{variant:"h3",align:"left"},"TextVQA Challenge 2019")),l.a.createElement("br",null),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Deadline: ",l.a.createElement(ia.a,{date:a,renderer:ya})),l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(Te.a,{container:!0},l.a.createElement(Te.a,{item:!0,xs:6,sm:4,lg:2},l.a.createElement(je.a,{variant:"h5",align:"left"},"Powered by:",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.cloudcv.org/web/challenges/challenge-page/244/"},l.a.createElement("img",{className:e.classes.bannerLogo+" "+e.classes.evalaiLogo,srcSet:"/assets/images/evalai_logo.png",alt:"EvalAI"})))))),l.a.createElement(Te.a,{item:!0,className:e.classes.overview,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"h4",align:"left"},"Overview"),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"TextVQA requires models to read and reason about text in an image to answer questions based on them. In order to perform well on this task, models need to first detect and read text in the images. Models then need to reason about this to answer the question."),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"Current state-of-the-art models fail to answer questions in TextVQA because they do not have text reading and reasoning capabilities. See the examples in the image to compare ground truth answers and corresponding predictions by a state-of-the-art model."),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"Challenge link: ",l.a.createElement(K.a,{className:e.classes.evalAILink,href:"https://evalai.cloudcv.org/web/challenges/challenge-page/244/"},"https://evalai.cloudcv.org/web/challenges/challenge-page/244/"))),l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12,md:4,lg:4},l.a.createElement("img",{className:e.classes.teaserImage,srcSet:"/assets/images/teaser.png",alt:"Teaser"})),l.a.createElement(Te.a,{item:!0,style:{marginTop:0},className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Results and Analysis"),l.a.createElement(je.a,{variant:"subtitle1",gutterBottom:!0,align:"left"},l.a.createElement("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/c-3E5WeIvw8",frameBorder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0}))),l.a.createElement(Te.a,{item:!0,style:{marginTop:0},className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Starter Code")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The starter code for TextVQA challenge is available in ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/"},"Pythia"),". Tutorial on how to submit a submission using ",l.a.createElement(K.a,{href:"https://arxiv.org/pdf/1904.08920"},"LoRRA")," model is available in ",l.a.createElement(K.a,{href:"https://learnpythia.readthedocs.io/en/latest/tutorials/challenge.html"},"documentation"),". LoRRA can be easily plugged to any VQA model to add text reading capabilities.")),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Prizes")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The winning team gets ",l.a.createElement(K.a,{href:"https://cloud.google.com/"},"Google Cloud Platform (GCP)")," credits worth $10k. Thank you for the generosity GCP!")),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Dates")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"4 March 2019")," \u2014 Challenge announced."),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"27 May 2019 (23:59:59 GMT)")," \u2014 Submission deadline for participants."),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"17 June 2019")," \u2014 Winners' announcment at the ",l.a.createElement(K.a,{href:"https://visualqa.org/workshop.html"},"Visual Question Answering and Dialog Workshop, CVPR 2019"),"."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Winners will be invited to give a short talk at the workshop. ",l.a.createElement("br",null),"For questions about the challenge, visit challenge's ",l.a.createElement(K.a,{href:"https://evalai-forum.cloudcv.org/c/text-vqa-2019"},"discussion board"),", join our ",l.a.createElement(K.a,{href:"https://groups.google.com/forum/#!forum/textvqa"},"Google Group")," or email us at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com"))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Dataset Description")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"You can find a detailed description and the download links for the dataset at the ",l.a.createElement(K.a,{href:"dataset"},"download")," page."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The challenge will be conducted on v0.5 of the ",l.a.createElement(K.a,{href:"dataset"},"TextVQA dataset"),", which is based on ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/web/index.html"},"OpenImages"),".",l.a.createElement("br",null),"TextVQA v0.5 contains 45,336 questions based on 28,408 images. The v0.5 training set contains 34,602 questions based on 21,953 images from OpenImages' training set. The v0.5 validation set contains 5,000 questions based on 3,166 images from OpenImages' training set while the v0.5 test-std set contains 5,734 questions based on 3,289 images from OpenImages' test set."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"To allow easier adoption, we also provide OCR tokens extracted using ",l.a.createElement(K.a,{href:"https://code.fb.com/ai-research/rosetta-understanding-text-in-images-and-videos-with-machine-learning/"},"Rosetta"),". Participants are free to use these OCR tokens and/or use other systems/ways to read/understand the text in the images.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Participation Guidelines")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Teams must register on ",l.a.createElement(K.a,{href:"https://evalai.cloudcv.org/"},"EvalAI")," and create a team for the challenge (",l.a.createElement(K.a,{href:"https://evalai.readthedocs.io/en/latest/participate.html"},"Quickstart"),")",l.a.createElement("br",null),"The challenge page is available at: ",l.a.createElement(K.a,{className:e.classes.evalAILink,href:"https://evalai.cloudcv.org/web/challenges/challenge-page/244/"},"https://evalai.cloudcv.org/web/challenges/challenge-page/244/")),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Challenge has two phases:")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,style:{width:"100%",overflowX:"auto"},xs:12,md:12,lg:12},l.a.createElement(oa.a,{style:{minWidth:700}},l.a.createElement(pa.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,l.a.createElement("b",null,"Phase")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Submissions")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Results")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Leaderboard")))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,l.a.createElement("b",null,"val")),l.a.createElement(ga.a,{align:"left"},"unlimited"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"none"))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,l.a.createElement("b",null,"test-std")),l.a.createElement(ga.a,{align:"left"},"5 total"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"public (optional)")))),l.a.createElement("br",null)),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"Please use the ",l.a.createElement("b",null,"validation")," split of TextVQA 0.5 for the ",l.a.createElement("b",null,"val")," phase and the ",l.a.createElement("b",null,"test")," split for the ",l.a.createElement("b",null,"test-std")," phase. While answers are already provided for the ",l.a.createElement("b",null,"validation")," set, this phase is useful for sanity checking the result format without wasting submissions in the other phases. For the ",l.a.createElement("b",null,"test-std")," phase, the results must be submitted on the full set. Submissions to ",l.a.createElement("b",null,"test-std")," phase are considered entries into the challenge. By default, the submissions for the ",l.a.createElement("b",null,"test-std")," phase are private but can be voluntarily released to the public leaderboard, with a limit of one public leaderboard entry per team. At the end of the challenge, the entry with best accuracy from each team will be made public automatically and will be used for the challenge rankings. We will contact the winning team to voluntarily present at the ",l.a.createElement(K.a,{href:"https://visualqa.org/workshop.html"},"Visual Question Answering and Dialog Workshop, CVPR 2019"),"."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"It is not acceptable to create multiple accounts for a single team in order to bypass the limits on number of submissions. The exception to this is if a group is working on multiple unrelated methods, in this case all sets of results can be submitted for evaluation. Results must be submitted to the evaluation server by the challenge deadline -- no exceptions will be made.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Submission Format")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"To submit to a phase, teams must upload a JSON file containing their model's answer prediction in the following format:"),l.a.createElement("br",null),l.a.createElement(je.a,{component:"span",align:"left",className:e.classes.preParent},l.a.createElement(je.a,{component:"pre"},l.a.createElement(je.a,{component:"span",variant:"body1"},l.a.createElement("code",null,JSON.stringify([{question_id:"INT",answer:"STRING"},{question_id:"...",answer:"..."}],null,2))))),l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"where ",l.a.createElement("b",null,"question_id")," is a question's unique id and ",l.a.createElement("b",null,"answer")," is the prediction by your model for the question. You can find an example submission file ",l.a.createElement(K.a,{href:"https://drive.google.com/file/d/1KpDGPUKILomUZY37b0N5urfMjF60eHNf/view?usp=sharing"},"here"),". When submitting, teams should also include a method name, method description, project URL, and publication URL if available.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{id:"evaluation",item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Evaluation")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"We use the same metric as VQA v2 which is robust to inter-human variability in phrasing the answers. More information on this can be found at ",l.a.createElement(K.a,{href:"https://visualqa.org/evaluation.html"},"https://visualqa.org/evaluation.html"))),l.a.createElement(Te.a,{id:"evaluation",item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Related Datasets")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{href:"https://rrc.cvc.uab.es/?ch=11"},"Scene-Text VQA")," is a concurrently released dataset that also contains questions about text in images. We encourage you to evaluate your approach on Scene-Text VQA as well, and/or consider using it as an additional source of training data.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Organizers")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement($e,{lgSize:3,people:fa[t][2019]})),l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignContent:"center",spacing:16,className:e.classes.sectionHeader},l.a.createElement(Te.a,{item:!0,xs:6,md:6,lg:4},l.a.createElement("img",{className:e.classes.bannerLogo,style:{marginTop:"5%"},srcSet:"/assets/images/fair_logo.png",alt:"Facebook Artificial Intelligence Research"})),l.a.createElement(Te.a,{item:!0,xs:3,md:3,lg:2},l.a.createElement("img",{className:e.classes.bannerLogo,srcSet:"/assets/images/gt_logo.png",alt:"Georgia Tech"}))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null))),l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)))}),ka=function(){return l.a.createElement("span",null,"Challenge has completed!")},Ca=function(e){var a=e.days,t=e.hours,n=e.minutes,s=e.seconds;return e.completed?l.a.createElement(ka,null):l.a.createElement("span",null,a," days ",t,"h ",n,"m ",s,"s")},Ta=function(e){return{title:{marginTop:"3em"},overview:{marginTop:"2em"},headings:{marginTop:"1em"},ulItems:{paddingLeft:"1.2em"},preParent:{backgroundColor:"#eee",border:"1px solid #ddd",borderRadius:"2px","& pre":{whiteSpace:"pre-wrap"},"& span":{padding:"1em"},"& code":{fontSize:"12px",color:"#000"}},evalAILink:{wordBreak:"break-word"},teaserImage:{width:"100%"},spanTypography:{display:"inline-block"},bannerLogo:{width:"100%"},sectionHeader:{marginTop:"0.35em"},evalaiLogo:{marginLeft:"-3px"}}},Ia=Object(g.withStyles)(Ta)(function(e){var a=xa.a.tz("2021-05-14T23:59:59","Etc/GMT").toDate(),t=-1===Object(p.e)().pathname.indexOf("textcaps")?"textvqa":"textcaps";return l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7},l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignItems:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12},l.a.createElement(je.a,{variant:"h3",align:"left"},"TextVQA Challenge 2021")),l.a.createElement("br",null),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Deadline: ",l.a.createElement(ia.a,{date:a,renderer:Ca})),l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(Te.a,{container:!0},l.a.createElement(Te.a,{item:!0,xs:6,sm:4,lg:2},l.a.createElement(je.a,{variant:"h5",align:"left"},"Powered by:",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.cloudcv.org/web/challenges/challenge-page/244/"},l.a.createElement("img",{className:e.classes.bannerLogo+" "+e.classes.evalaiLogo,srcSet:"/assets/images/evalai_logo.png",alt:"EvalAI"})))))),l.a.createElement(Te.a,{item:!0,className:e.classes.overview,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"h4",align:"left"},"Overview"),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"TextVQA requires models to read and reason about text in an image to answer questions based on them. In order to perform well on this task, models need to first detect and read text in the images. Models then need to reason about this to answer the question."),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"Current state-of-the-art models fail to answer questions in TextVQA because they do not have text reading and reasoning capabilities. See the examples in the image to compare ground truth answers and corresponding predictions by a state-of-the-art model."),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"Challenge link: ",l.a.createElement(K.a,{className:e.classes.evalAILink,target:"_blank",href:"https://eval.ai/web/challenges/challenge-page/874/"},"https://eval.ai/web/challenges/challenge-page/874/"))),l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12,md:4,lg:4},l.a.createElement("img",{className:e.classes.teaserImage,srcSet:"/assets/images/teaser.png",alt:"Teaser"})),l.a.createElement(Te.a,{item:!0,style:{marginTop:0},className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Starter Code")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The starter code for TextVQA challenge is available in ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/"},"Pythia"),". Tutorial on how to submit a submission using ",l.a.createElement(K.a,{href:"https://arxiv.org/pdf/1904.08920"},"LoRRA")," model is available in ",l.a.createElement(K.a,{href:"https://learnpythia.readthedocs.io/en/latest/tutorials/challenge.html"},"documentation"),". LoRRA can be easily plugged to any VQA model to add text reading capabilities. Use ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/tree/project/m4c"},"project/m4c")," branch to use TextVQA SoTA model, M4C. Find more details on how to use it at ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/tree/project/m4c/projects/M4C"},"this link")," and ",l.a.createElement(K.a,{href:"https://arxiv.org/abs/1911.06258"},"read the paper"),".")),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Dates")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"9 March 2021")," \u2014 Challenge announced."),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"14 May 2021 (23:59:59 GMT)")," \u2014 Submission deadline for participants."),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"June 19th 2021")," \u2014 Winners' announcment at the ",l.a.createElement(K.a,{href:"https://visualqa.org/workshop.html"},"Visual Question Answering Workshop, CVPR 2021"),"."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Winners will be invited to give a short talk at the workshop. ",l.a.createElement("br",null),"For questions about the challenge, join our ",l.a.createElement(K.a,{href:"https://groups.google.com/forum/#!forum/textvqa"},"Google Group")," or email us at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"}," textvqa@fb.com."))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Dataset Description")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"You can find a detailed description and the download links for the dataset at the ",l.a.createElement(K.a,{href:"/dataset"},"download")," page."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The challenge will be conducted on v0.5.1 of the ",l.a.createElement(K.a,{href:"dataset"},"TextVQA dataset"),", which is based on ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/web/index.html"},"OpenImages"),".",l.a.createElement("br",null),"TextVQA v0.5.1 contains 45,336 questions based on 28,408 images. The v0.5.1 training set contains 34,602 questions based on 21,953 images from OpenImages' training set. The v0.5.1 validation set contains 5,000 questions based on 3,166 images from OpenImages' training set while the v0.5.1 test-std set contains 5,734 questions based on 3,289 images from OpenImages' test set."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"To allow easier adoption, we also provide OCR tokens extracted using ",l.a.createElement(K.a,{href:"https://code.fb.com/ai-research/rosetta-understanding-text-in-images-and-videos-with-machine-learning/"},"Rosetta"),". Participants are free to use these OCR tokens and/or use other systems/ways to read/understand the text in the images.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Participation Guidelines")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Teams must register on ",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.cloudcv.org/"},"EvalAI")," and create a team for the challenge (",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.readthedocs.io/en/latest/participate.html"},"Quickstart"),")",l.a.createElement("br",null),"The challenge page is available at: ",l.a.createElement(K.a,{className:e.classes.evalAILink,target:"_blank",href:"https://eval.ai/web/challenges/challenge-page/874/"},"https://eval.ai/web/challenges/challenge-page/874/")),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Challenge has three phases:")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,style:{width:"100%",overflowX:"auto"},xs:12,md:12,lg:12},l.a.createElement(oa.a,{style:{minWidth:700}},l.a.createElement(pa.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"No."),l.a.createElement(ga.a,null,l.a.createElement("b",null,"Phase")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Submissions")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Results")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Leaderboard")))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#1"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"val")),l.a.createElement(ga.a,{align:"left"},"unlimited"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"none"))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#2"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"test-std")),l.a.createElement(ga.a,{align:"left"},"5 total combined with #3"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"public (optional)"))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#3"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"test-violating-standard-guidelines")),l.a.createElement(ga.a,{align:"left"},"5 total combined with #2"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"none")))),l.a.createElement("br",null)),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"Please use the ",l.a.createElement("b",null,"validation")," split of TextVQA 0.5.1 for the ",l.a.createElement("b",null,"val")," phase and the ",l.a.createElement("b",null,"test")," split for the ",l.a.createElement("b",null,"test-std")," phase. While answers are already provided for the ",l.a.createElement("b",null,"validation")," set, this phase is useful for sanity checking the result format without wasting submissions in the other phases. For the ",l.a.createElement("b",null,"test-std")," phase, the results must be submitted on the full set. Submissions to ",l.a.createElement("b",null,"test-std")," phase are considered entries into the challenge. By default, the submissions for the ",l.a.createElement("b",null,"test-std")," phase are private but can be voluntarily released to the public leaderboard, with a limit of one public leaderboard entry per team. At the end of the challenge, the entry with best accuracy from each team will be made public automatically and will be used for the challenge rankings. If you make an entry to ",l.a.createElement("b",null,"test-std")," and do not want to be considered for the challenge, please contact us at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com")," We will contact the winning team to voluntarily present at the ",l.a.createElement(K.a,{href:"https://visualqa.org/workshop.html"},"Visual Question Answering and Dialog Workshop, CVPR 2021"),". Following guidelines must be followed for making a submission to ",l.a.createElement("b",null,"test-std"),".",l.a.createElement("b",null,"Note:")," Teams submitting to the challenge are required to submit a up-to 2 page abstract after the challenge. A detailed report on challenge analysis citing these abstracts will be provided later"),l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. Submission ",l.a.createElement("b",null,"should not use ensembles")," of any form for making predictions ",l.a.createElement("i",null,"i.e.")," it needs to be a single model.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Submission ",l.a.createElement("b",null,"should not use ground-truth OCRs")," or any kind of human-annotated text tokens for training or for making predictions. Note that submissions are allowed to use any kind of automatic OCR system.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. Total number of submissions from a participant team on ",l.a.createElement("b",null,"test-std")," and ",l.a.createElement("b",null,"test-violating-standard-guidelines")," (more details on this new phase below) combined should not exceed ",l.a.createElement("b",null,"5"),".")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Only one submission can be made public on ",l.a.createElement("b",null,"test-std")," leaderboard by one participant team."))),l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},l.a.createElement("b",null,"Note:")," Similar to last year, we have a new phase called ",l.a.createElement("b",null,"test-violating-standard-guidelines")," which participants can to submit for submissions that violate above guidelines. This phase doesn't have a leaderboard and submissions on this phase will remain private. The total number of submissions for ",l.a.createElement("b",null,"test-std")," and ",l.a.createElement("b",null,"test-violating-standard-guidelines")," combined should not exceed 5."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"It is not acceptable to create multiple accounts for a single team in order to bypass the limits on number of submissions. A person can only be part of one team. The exception to this is if a group is working on multiple unrelated methods, in this case all sets of results can be submitted for evaluation. Results must be submitted to the evaluation server by the challenge deadline -- no exceptions will be made.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Abstract Submission Guidelines")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"All participants of the TextVQA and TextCaps Challenges can submit a short abstract (max 2 pages in CVPR camera ready format, references excluded) providing details of their submission. To be considered as challenge winner the submission of an abstract is required. The abstract must be submitted by ",l.a.createElement("b",null,"Friday, May 21th, 4pm PDT.")," Submit by email to textvqa@fb.com.",l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. The submission is NOT blind.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Please include all team members/authors names and their affiliation.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. Please include the team name used on the challenge server so we can correctly identify your results.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Please let us know in the email if you are fine that your abstract PDF will be on the web page of the challenge."))),"Recommended content includes:",l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. List of all the data sources which were used for training")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Experimental setup including training details, what pretraining was used (if any), how OCR tokens were incorporated")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. A table with ablations (on validation set)")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Discussion on the insights you gained."))))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Submission Format")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"To submit to a phase, teams must upload a JSON file containing their model's answer prediction in the following format:"),l.a.createElement("br",null),l.a.createElement(je.a,{component:"span",align:"left",className:e.classes.preParent},l.a.createElement(je.a,{component:"pre"},l.a.createElement(je.a,{component:"span",variant:"body1"},l.a.createElement("code",null,JSON.stringify([{question_id:"INT",answer:"STRING"},{question_id:"...",answer:"..."}],null,2))))),l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"where ",l.a.createElement("b",null,"question_id")," is a question's unique id and ",l.a.createElement("b",null,"answer")," is the prediction by your model for the question. You can find an example submission file ",l.a.createElement(K.a,{href:"https://drive.google.com/file/d/1KpDGPUKILomUZY37b0N5urfMjF60eHNf/view?usp=sharing"},"here"),". When submitting, teams should also include a method name, method description, project URL, and publication URL if available.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{id:"evaluation",item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Evaluation")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"We use the same metric as VQA v2 which is robust to inter-human variability in phrasing the answers. More information on this can be found at ",l.a.createElement(K.a,{href:"https://visualqa.org/evaluation.html"},"https://visualqa.org/evaluation.html"))),l.a.createElement(Te.a,{id:"related",item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Related Datasets")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Following is a list of datasets related or similar to TextVQA. We encourage you to evaluate your approach on these as well, and/or consider using it as an additional source of training data. ",l.a.createElement("br",null),"1. ",l.a.createElement(K.a,{href:"https://rrc.cvc.uab.es/?ch=11"},"Scene-Text VQA")," is a concurrently released dataset that also contains questions about text in images. Scene-Text VQA is available in M4C branch of Pythia. ",l.a.createElement("br",null),"2. ",l.a.createElement(K.a,{href:"https://ocr-vqa.github.io/"},"OCR-VQA")," is another TextVQA dataset which provides questions inquiring about title, author, edition, year and genre of the book and corresponding ground-truth answer. OCR-VQA is also available in M4c branch of Pythia. ",l.a.createElement("br",null),"3. ",l.a.createElement(K.a,{href:"https://textvqa.org/textcaps"},"TextCaps")," dataset is based on task of image captioning with reading comprehension. The captions require reading text in the image and are collected on the same images as TextVQA.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Organizers")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement($e,{lgSize:3,people:fa[t][2021]})),l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignContent:"center",spacing:16,className:e.classes.sectionHeader},l.a.createElement(Te.a,{item:!0,xs:6,md:6,lg:4},l.a.createElement("img",{className:e.classes.bannerLogo,style:{marginTop:"5%"},srcSet:"/assets/images/fair_logo.png",alt:"Facebook Artificial Intelligence Research"})),l.a.createElement(Te.a,{item:!0,xs:3,md:3,lg:2},l.a.createElement("img",{className:e.classes.bannerLogo,srcSet:"/assets/images/gt_logo.png",alt:"Georgia Tech"}))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null))),l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)))}),_a=function(){return l.a.createElement("span",null,"Challenge has completed!")},Ra=function(e){var a=e.days,t=e.hours,n=e.minutes,s=e.seconds;return e.completed?l.a.createElement(_a,null):l.a.createElement("span",null,a," days ",t,"h ",n,"m ",s,"s")},Oa=Object(g.withStyles)(function(e){return{title:{marginTop:"3em"},overview:{marginTop:"2em"},headings:{marginTop:"1em"},ulItems:{paddingLeft:"1.2em"},preParent:{backgroundColor:"#eee",border:"1px solid #ddd",borderRadius:"2px","& pre":{whiteSpace:"pre-wrap"},"& span":{padding:"1em"},"& code":{fontSize:"12px",color:"#000"}},evalAILink:{wordBreak:"break-word"},teaserImage:{width:"100%"},spanTypography:{display:"inline-block"},bannerLogo:{width:"100%"},sectionHeader:{marginTop:"0.35em"},evalaiLogo:{marginLeft:"-3px"}}})(function(e){var a=xa.a.tz("2020-05-15T23:59:59","Etc/GMT").toDate(),t=-1===Object(p.e)().pathname.indexOf("textcaps")?"textvqa":"textcaps";return l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7},l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignItems:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12},l.a.createElement(je.a,{variant:"h3",align:"left"},"TextVQA Challenge 2020")),l.a.createElement("br",null),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Deadline: ",l.a.createElement(ia.a,{date:a,renderer:Ra})),l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(Te.a,{container:!0},l.a.createElement(Te.a,{item:!0,xs:6,sm:4,lg:2},l.a.createElement(je.a,{variant:"h5",align:"left"},"Powered by:",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.cloudcv.org/web/challenges/challenge-page/244/"},l.a.createElement("img",{className:e.classes.bannerLogo+" "+e.classes.evalaiLogo,srcSet:"/assets/images/evalai_logo.png",alt:"EvalAI"})))))),l.a.createElement(Te.a,{item:!0,className:e.classes.overview,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"h4",align:"left"},"Overview"),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"TextVQA requires models to read and reason about text in an image to answer questions based on them. In order to perform well on this task, models need to first detect and read text in the images. Models then need to reason about this to answer the question."),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"Current state-of-the-art models fail to answer questions in TextVQA because they do not have text reading and reasoning capabilities. See the examples in the image to compare ground truth answers and corresponding predictions by a state-of-the-art model."),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"Challenge link: ",l.a.createElement(K.a,{className:e.classes.evalAILink,target:"_blank",href:"https://evalai.cloudcv.org/web/challenges/challenge-page/551/"},"https://evalai.cloudcv.org/web/challenges/challenge-page/551/"))),l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12,md:4,lg:4},l.a.createElement("img",{className:e.classes.teaserImage,srcSet:"/assets/images/teaser.png",alt:"Teaser"})),l.a.createElement(Te.a,{item:!0,style:{marginTop:0},className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Results and Analysis"),l.a.createElement(je.a,{variant:"subtitle1",gutterBottom:!0,align:"left"},l.a.createElement("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/O5y8i3OYdo8",frameBorder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0}))),l.a.createElement(Te.a,{item:!0,style:{marginTop:0},className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Starter Code")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The starter code for TextVQA challenge is available in ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/"},"Pythia"),". Tutorial on how to submit a submission using ",l.a.createElement(K.a,{href:"https://arxiv.org/pdf/1904.08920"},"LoRRA")," model is available in ",l.a.createElement(K.a,{href:"https://learnpythia.readthedocs.io/en/latest/tutorials/challenge.html"},"documentation"),". LoRRA can be easily plugged to any VQA model to add text reading capabilities. Use ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/tree/project/m4c"},"project/m4c")," branch to use TextVQA SoTA model, M4C. Find more details on how to use it at ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/tree/project/m4c/projects/M4C"},"this link")," and ",l.a.createElement(K.a,{href:"https://arxiv.org/abs/1911.06258"},"read the paper"),".")),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Prizes")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"We will be providing the winners of the second TextVQA 2020 challenge, the first TextCaps 2020 challenge (coming soon) and the first joint TextVQA and TextCaps 2020 challenge (coming soon) ",l.a.createElement(K.a,{href:"https://cloud.google.com/"},"Google Cloud Platform (GCP)"),"  credits worth $10k total. We thank GCP for their generosity.")),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Dates")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"13 March 2020")," \u2014 Challenge announced."),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"15 May 2020 (23:59:59 GMT)")," \u2014 Submission deadline for participants."),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"June 14th 2020")," \u2014 Winners' announcment at the ",l.a.createElement(K.a,{href:"https://visualqa.org/workshop.html"},"Visual Question Answering and Dialog Workshop, CVPR 2020"),"."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Winners will be invited to give a short talk at the workshop. ",l.a.createElement("br",null),"For questions about the challenge, join our ",l.a.createElement(K.a,{href:"https://groups.google.com/forum/#!forum/textvqa"},"Google Group")," or email us at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"}," textvqa@fb.com."))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Dataset Description")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"You can find a detailed description and the download links for the dataset at the ",l.a.createElement(K.a,{href:"/dataset"},"download")," page."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The challenge will be conducted on v0.5.1 of the ",l.a.createElement(K.a,{href:"dataset"},"TextVQA dataset"),", which is based on ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/web/index.html"},"OpenImages"),".",l.a.createElement("br",null),"TextVQA v0.5.1 contains 45,336 questions based on 28,408 images. The v0.5.1 training set contains 34,602 questions based on 21,953 images from OpenImages' training set. The v0.5.1 validation set contains 5,000 questions based on 3,166 images from OpenImages' training set while the v0.5.1 test-std set contains 5,734 questions based on 3,289 images from OpenImages' test set."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"To allow easier adoption, we also provide OCR tokens extracted using ",l.a.createElement(K.a,{href:"https://code.fb.com/ai-research/rosetta-understanding-text-in-images-and-videos-with-machine-learning/"},"Rosetta"),". Participants are free to use these OCR tokens and/or use other systems/ways to read/understand the text in the images.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Participation Guidelines")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Teams must register on ",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.cloudcv.org/"},"EvalAI")," and create a team for the challenge (",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.readthedocs.io/en/latest/participate.html"},"Quickstart"),")",l.a.createElement("br",null),"The challenge page is available at: ",l.a.createElement(K.a,{className:e.classes.evalAILink,target:"_blank",href:"https://evalai.cloudcv.org/web/challenges/challenge-page/551/"},"https://evalai.cloudcv.org/web/challenges/challenge-page/551/")),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Challenge has three phases:")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,style:{width:"100%",overflowX:"auto"},xs:12,md:12,lg:12},l.a.createElement(oa.a,{style:{minWidth:700}},l.a.createElement(pa.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"No."),l.a.createElement(ga.a,null,l.a.createElement("b",null,"Phase")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Submissions")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Results")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Leaderboard")))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#1"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"val")),l.a.createElement(ga.a,{align:"left"},"unlimited"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"none"))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#2"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"test-std")),l.a.createElement(ga.a,{align:"left"},"5 total combined with #3"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"public (optional)"))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#3"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"test-violating-standard-guidelines")),l.a.createElement(ga.a,{align:"left"},"5 total combined with #2"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"none")))),l.a.createElement("br",null)),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"Please use the ",l.a.createElement("b",null,"validation")," split of TextVQA 0.5.1 for the ",l.a.createElement("b",null,"val")," phase and the ",l.a.createElement("b",null,"test")," split for the ",l.a.createElement("b",null,"test-std")," phase. While answers are already provided for the ",l.a.createElement("b",null,"validation")," set, this phase is useful for sanity checking the result format without wasting submissions in the other phases. For the ",l.a.createElement("b",null,"test-std")," phase, the results must be submitted on the full set. Submissions to ",l.a.createElement("b",null,"test-std")," phase are considered entries into the challenge. By default, the submissions for the ",l.a.createElement("b",null,"test-std")," phase are private but can be voluntarily released to the public leaderboard, with a limit of one public leaderboard entry per team. At the end of the challenge, the entry with best accuracy from each team will be made public automatically and will be used for the challenge rankings. If you make an entry to ",l.a.createElement("b",null,"test-std")," and do not want to be considered for the challenge, please contact us at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com")," We will contact the winning team to voluntarily present at the ",l.a.createElement(K.a,{href:"https://visualqa.org/workshop.html"},"Visual Question Answering and Dialog Workshop, CVPR 2020"),". Following guidelines must be followed for making a submission to ",l.a.createElement("b",null,"test-std"),".",l.a.createElement("b",null,"Note:")," Teams submitting to the challenge are required to submit a up-to 2 page abstract after the challenge. A detailed report on challenge analysis citing these abstracts will be provided later"),l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. Submission ",l.a.createElement("b",null,"should not use ensembles")," of any form for making predictions ",l.a.createElement("i",null,"i.e.")," it needs to be a single model.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Submission ",l.a.createElement("b",null,"should not use ground-truth OCRs")," or any kind of human-annotated text tokens for training or for making predictions. Note that submissions are allowed to use any kind of automatic OCR system.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. Total number of submissions from a participant team on ",l.a.createElement("b",null,"test-std")," and ",l.a.createElement("b",null,"test-violating-standard-guidelines")," (more details on this new phase below) combined should not exceed ",l.a.createElement("b",null,"5"),".")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Only one submission can be made public on ",l.a.createElement("b",null,"test-std")," leaderboard by one participant team."))),l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},l.a.createElement("b",null,"NEW:")," This year we have a new phase called ",l.a.createElement("b",null,"test-violating-standard-guidelines")," which participants can to submit for submissions that violate above guidelines. This phase doesn't have a leaderboard and submissions on this phase will remain private. The total number of submissions for ",l.a.createElement("b",null,"test-std")," and ",l.a.createElement("b",null,"test-violating-standard-guidelines")," combined should not exceed 5."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"It is not acceptable to create multiple accounts for a single team in order to bypass the limits on number of submissions. A person can only be part of one team. The exception to this is if a group is working on multiple unrelated methods, in this case all sets of results can be submitted for evaluation. Results must be submitted to the evaluation server by the challenge deadline -- no exceptions will be made.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Abstract Submission Guidelines")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"All participants of the TextVQA and TextCaps Challenges can submit a short abstract (max 2 pages in CVPR camera ready format, references excluded) providing details of their submission. To be considered as challenge winner the submission of an abstract is required. The abstract must be submitted by ",l.a.createElement("b",null,"Wednesday, May 20th, 4pm PDT.")," Submit by email to textvqa@fb.com.",l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. The submission is NOT blind.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Please include all team members/authors names and their affiliation.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. Please include the team name used on the challenge server so we can correctly identify your results.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Please let us know in the email if you are fine that your abstract PDF will be on the web page of the challenge."))),"Recommended content includes:",l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. List of all the data sources which were used for training")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Experimental setup including training details, what pretraining was used (if any), how OCR tokens were incorporated")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. A table with ablations (on validation set)")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Discussion on the insights you gained."))))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Submission Format")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"To submit to a phase, teams must upload a JSON file containing their model's answer prediction in the following format:"),l.a.createElement("br",null),l.a.createElement(je.a,{component:"span",align:"left",className:e.classes.preParent},l.a.createElement(je.a,{component:"pre"},l.a.createElement(je.a,{component:"span",variant:"body1"},l.a.createElement("code",null,JSON.stringify([{question_id:"INT",answer:"STRING"},{question_id:"...",answer:"..."}],null,2))))),l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"where ",l.a.createElement("b",null,"question_id")," is a question's unique id and ",l.a.createElement("b",null,"answer")," is the prediction by your model for the question. You can find an example submission file ",l.a.createElement(K.a,{href:"https://drive.google.com/file/d/1KpDGPUKILomUZY37b0N5urfMjF60eHNf/view?usp=sharing"},"here"),". When submitting, teams should also include a method name, method description, project URL, and publication URL if available.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{id:"evaluation",item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Evaluation")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"We use the same metric as VQA v2 which is robust to inter-human variability in phrasing the answers. More information on this can be found at ",l.a.createElement(K.a,{href:"https://visualqa.org/evaluation.html"},"https://visualqa.org/evaluation.html"))),l.a.createElement(Te.a,{id:"related",item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Related Datasets")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Following is a list of datasets related or similar to TextVQA. We encourage you to evaluate your approach on these as well, and/or consider using it as an additional source of training data. ",l.a.createElement("br",null),"1. ",l.a.createElement(K.a,{href:"https://rrc.cvc.uab.es/?ch=11"},"Scene-Text VQA")," is a concurrently released dataset that also contains questions about text in images. Scene-Text VQA is available in M4C branch of Pythia. ",l.a.createElement("br",null),"2. ",l.a.createElement(K.a,{href:"https://ocr-vqa.github.io/"},"OCR-VQA")," is another TextVQA dataset which provides questions inquiring about title, author, edition, year and genre of the book and corresponding ground-truth answer. OCR-VQA is also available in M4c branch of Pythia. ",l.a.createElement("br",null),"3. ",l.a.createElement(K.a,{href:"https://textvqa.org/textcaps"},"TextCaps")," dataset is based on task of image captioning with reading comprehension. The captions require reading text in the image and are collected on the same images as TextVQA.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Organizers")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement($e,{lgSize:3,people:fa[t][2020]})),l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignContent:"center",spacing:16,className:e.classes.sectionHeader},l.a.createElement(Te.a,{item:!0,xs:6,md:6,lg:4},l.a.createElement("img",{className:e.classes.bannerLogo,style:{marginTop:"5%"},srcSet:"/assets/images/fair_logo.png",alt:"Facebook Artificial Intelligence Research"})),l.a.createElement(Te.a,{item:!0,xs:3,md:3,lg:2},l.a.createElement("img",{className:e.classes.bannerLogo,srcSet:"/assets/images/gt_logo.png",alt:"Georgia Tech"}))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null))),l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)))}),Aa=Object(g.withStyles)(Ta)(function(e){var a=xa.a.tz("2021-05-14T23:59:59","Etc/GMT").toDate(),t=-1===Object(p.e)().pathname.indexOf("textcaps")?"textvqa":"textcaps";return l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7},l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignItems:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12},l.a.createElement(je.a,{variant:"h3",align:"left"},"TextCaps Challenge 2021")),l.a.createElement("br",null),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Deadline: ",l.a.createElement(ia.a,{date:a,renderer:Ca})),l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(Te.a,{container:!0},l.a.createElement(Te.a,{item:!0,xs:6,sm:4,lg:2},l.a.createElement(je.a,{variant:"h5",align:"left"},"Powered by:",l.a.createElement(K.a,{target:"_blank",href:"https://eval.ai/web/challenges/challenge-page/906"},l.a.createElement("img",{className:e.classes.bannerLogo+" "+e.classes.evalaiLogo,srcSet:"/assets/images/evalai_logo.png",alt:"EvalAI"})))))),l.a.createElement(Te.a,{item:!0,className:e.classes.overview,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"h4",align:"left"},"Overview"),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"TextCaps requires models to read and reason about text in images to generate captions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it and visual content in the image to generate image descriptions."),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"Current state-of-the-art models fail to generate captions for images in TextCaps because they do not have text reading and reasoning capabilities. See the examples in the image to compare ground truth answers and corresponding predictions by a state-of-the-art model."),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"Challenge link: ",l.a.createElement(K.a,{className:e.classes.evalAILink,target:"_blank",href:"https://eval.ai/web/challenges/challenge-page/906"},"https://eval.ai/web/challenges/challenge-page/906"))),l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12,md:4,lg:4},l.a.createElement("img",{className:e.classes.teaserImage,srcSet:"textcaps"===t?"/assets/images/textcaps_teaser.jpg":"/assets/images/teaser.png",alt:"Teaser"})),l.a.createElement(Te.a,{item:!0,style:{marginTop:0},className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Starter Code")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Starter code is available in Pythia framework. Use ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/tree/project/m4c/"},"project/m4c")," branch to use TextCaps SoTA model, M4C. Find more details on how to use it at ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/tree/project/m4c/projects/M4C_Captioner/"},"this link")," and ",l.a.createElement(K.a,{href:"https://arxiv.org/abs/1911.06258"},"read the paper"),".")),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Dates")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"31 March 2021")," \u2014 Challenge announced."),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"14 May 2021 (23:59:59 GMT)")," \u2014 Submission deadline for participants."),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"June 19th 2021")," \u2014 Winners' announcment at the ",l.a.createElement(K.a,{href:"https://visualqa.org/workshop.html"},"Visual Question Answering Workshop, CVPR 2021"),"."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Winners will be invited to give a short talk at the workshop. ",l.a.createElement("br",null),"For questions about the challenge, join our ",l.a.createElement(K.a,{href:"https://groups.google.com/forum/#!forum/textvqa"},"Google Group")," or email us at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com"))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Dataset Description")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"You can find a detailed description and the download links for the dataset at the ",l.a.createElement(K.a,{href:"dataset"},"download")," page."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The challenge will be conducted on v0.1 of the ",l.a.createElement(K.a,{href:"/textcaps/dataset"},"TextCaps dataset"),", which is based on ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/web/index.html"},"OpenImages"),".",l.a.createElement("br",null),"TextCaps v0.1 contains 142,040 captions based on 28,408 images. The v0.1 training set contains 109,765 captions based on 21,953 images from OpenImages' training set. The v0.1 validation set contains 15,830 captions based on 3,166 images from OpenImages' training set while the v0.1 test-std set contains 16,445 captions based on 3,289 images from OpenImages' test set."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"To allow easier adoption, we also provide OCR tokens extracted using ",l.a.createElement(K.a,{href:"https://code.fb.com/ai-research/rosetta-understanding-text-in-images-and-videos-with-machine-learning/"},"Rosetta"),". Participants are free to use these OCR tokens and/or use other systems/ways to read/understand the text in the images.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Participation Guidelines")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Teams must register on ",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.cloudcv.org/"},"EvalAI")," and create a team for the challenge (",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.readthedocs.io/en/latest/participate.html"},"Quickstart"),")",l.a.createElement("br",null),"The challenge page is available at: ",l.a.createElement(K.a,{className:e.classes.evalAILink,href:"https://eval.ai/web/challenges/challenge-page/906"},"https://eval.ai/web/challenges/challenge-page/906")),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Challenge has three phases:")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,style:{width:"100%",overflowX:"auto"},xs:12,md:12,lg:12},l.a.createElement(oa.a,{style:{minWidth:700}},l.a.createElement(pa.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"No."),l.a.createElement(ga.a,null,l.a.createElement("b",null,"Phase")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Submissions")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Results")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Leaderboard")))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#1"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"val")),l.a.createElement(ga.a,{align:"left"},"unlimited"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"none"))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#2"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"test-std")),l.a.createElement(ga.a,{align:"left"},"5 total combined with #3"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"public (optional)"))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#3"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"test-violating-standard-guidelines")),l.a.createElement(ga.a,{align:"left"},"5 total combined with #2"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"none")))),l.a.createElement("br",null)),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"Please use the ",l.a.createElement("b",null,"validation")," split of TextCaps for the ",l.a.createElement("b",null,"val")," phase and the ",l.a.createElement("b",null,"test")," split for the ",l.a.createElement("b",null,"test-std")," phase. While answers are already provided for the ",l.a.createElement("b",null,"validation")," set, this phase is useful for sanity checking the result format without wasting submissions in the other phases. For the ",l.a.createElement("b",null,"test-std")," phase, the results must be submitted on the full set. Submissions to ",l.a.createElement("b",null,"test-std")," phase are considered entries into the challenge. By default, the submissions for the ",l.a.createElement("b",null,"test-std")," phase are private but can be voluntarily released to the public leaderboard, with a limit of one public leaderboard entry per team. At the end of the challenge, the entry with best accuracy from each team will be made public automatically and will be used for the challenge rankings. If you make an entry to ",l.a.createElement("b",null,"test-std")," and do not want to be considered for the challenge, please contact us at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com"),". We will contact the winning team to voluntarily present at the ",l.a.createElement(K.a,{href:"https://visualqa.org/workshop.html"},"Visual Question Answering and Dialog Workshop, CVPR 2021"),". Following guidelines must be followed for making a submission to ",l.a.createElement("b",null,"test-std"),".",l.a.createElement("b",null,"Note:")," Teams submitting to the challenge are required to submit a up-to 2 page abstract after the challenge. A detailed report on challenge analysis citing these abstracts will be provided later"),l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. Submission ",l.a.createElement("b",null,"should not use ensembles")," of any form for making predictions ",l.a.createElement("i",null,"i.e.")," it needs to be a single model.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Submission ",l.a.createElement("b",null,"should not use ground-truth OCRs")," or any kind of human-annotated text tokens for training or for making predictions. Note that submissions are allowed to use any kind of automatic OCR system. .")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. Total number of submissions from a participant team on ",l.a.createElement("b",null,"test-std")," and ",l.a.createElement("b",null,"test-violating-standard-guidelines")," (more details on this new phase below) combined should not exceed ",l.a.createElement("b",null,"5"),".")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Only one submission can be made public on ",l.a.createElement("b",null,"test-std")," leaderboard by one participant team."))),l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"Participants can submit to phase ",l.a.createElement("b",null,"test-violating-standard-guidelines")," for submissions that violate above guidelines. This phase doesn't have a leaderboard and submissions on this phase will remain private. The total number of submissions for ",l.a.createElement("b",null,"test-std")," and ",l.a.createElement("b",null,"test-violating-standard-guidelines")," combined should not exceed 5."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"It is not acceptable to create multiple accounts for a single team in order to bypass the limits on number of submissions. A person can only be part of one particular team. The exception to this is if a group is working on multiple unrelated methods, in this case all sets of results can be submitted for evaluation. Results must be submitted to the evaluation server by the challenge deadline -- no exceptions will be made.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Abstract Submission Guidelines")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"All participants of the TextVQA and TextCaps Challenges can submit a short abstract (max 2 pages in CVPR camera ready format, references excluded) providing details of their submission. To be considered as challenge winner the submission of an abstract is required. The abstract must be submitted by ",l.a.createElement("b",null,"Wednesday, May 20th, 4pm PDT.")," Submit by email to textvqa@fb.com.",l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. The submission is NOT blind.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Please include all team members/authors names and their affiliation.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. Please include the team name used on the challenge server so we can correctly identify your results.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Please let us know in the email if you are fine that your abstract PDF will be on the web page of the challenge."))),"Recommended content includes:",l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. List of all the data sources which were used for training")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Experimental setup including training details, what pretraining was used (if any), how OCR tokens were incorporated")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. A table with ablations (on validation set)")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Discussion on the insights you gained."))))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Submission Format")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"To submit to a phase, teams must upload a JSON file containing their model's answer prediction in the following format:"),l.a.createElement("br",null),l.a.createElement(je.a,{component:"span",align:"left",className:e.classes.preParent},l.a.createElement(je.a,{component:"pre"},l.a.createElement(je.a,{component:"span",variant:"body1"},l.a.createElement("code",null,JSON.stringify([{image_id:"STRING",caption:"STRING"},{image_id:"...",caption:"..."}],null,2))))),l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"where ",l.a.createElement("b",null,"image_id")," is a image's unique id and ",l.a.createElement("b",null,"caption")," is the prediction by your model for the image. You can find an example submission file ",l.a.createElement(K.a,{href:"https://drive.google.com/file/d/1KpDGPUKILomUZY37b0N5urfMjF60eHNf/view?usp=sharing"},"here"),". When submitting, teams should also include a method name, method description, project URL, and publication URL if available.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{id:"evaluation",item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Evaluation")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"We report BLEU, METEOR, ROUGE-L, SPICE and CIDEr metrics for each submissions using ",l.a.createElement(K.a,{href:"https://github.com/tylin/coco-caption"},"coco-captions")," package. The leaderboard scores are sorted based on CIDEr and final ranking will be provided after the challenge based on a mixture of human evaluations and CIDEr.")),l.a.createElement(Te.a,{id:"evaluation",item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Related Datasets")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Following is a list of datasets related or similar to TextCaps. We encourage you to evaluate your approach on these as well, and/or consider using it as an additional source of training data. ",l.a.createElement("br",null),"1. ",l.a.createElement(K.a,{href:"/"},"TextVQA")," dataset is based on task of answering a question about an image. The questions require reading text in the image to be answered and are collected on the same images as TextVQA.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Organizers")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement($e,{lgSize:3,people:fa[t][2021]})),l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignContent:"center",spacing:16,className:e.classes.sectionHeader},l.a.createElement(Te.a,{item:!0,xs:6,md:6,lg:4},l.a.createElement(K.a,{target:"_blank",href:"https://research.fb.com/category/facebook-ai-research/"},l.a.createElement("img",{className:e.classes.bannerLogo,style:{marginTop:"5%"},srcSet:"/assets/images/fair_logo.png",alt:"Facebook Artificial Intelligence Research"}))),l.a.createElement(Te.a,{item:!0,xs:3,md:3,lg:2},l.a.createElement(K.a,{target:"_blank",href:"https://www.gatech.edu/"},l.a.createElement("img",{className:e.classes.bannerLogo,srcSet:"/assets/images/gt_logo.png",alt:"Georgia Tech"}))),l.a.createElement(Te.a,{item:!0,xs:3,md:3,lg:2},l.a.createElement(K.a,{target:"_blank",href:"https://www.berkeley.edu/"},l.a.createElement("img",{className:e.classes.bannerLogo,style:{marginTop:"5%"},srcSet:"https://www.berkeley.edu/images/uploads/logo-ucberkeley.png",alt:"UC Berkeley"})))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null))),l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)))}),Sa=Object(g.withStyles)(Ta)(function(e){var a=xa.a.tz("2020-05-18T23:59:59","Etc/GMT").toDate(),t=-1===Object(p.e)().pathname.indexOf("textcaps")?"textvqa":"textcaps";return l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7},l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignItems:"flex-start",spacing:16},l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12},l.a.createElement(je.a,{variant:"h3",align:"left"},"TextCaps Challenge 2020")),l.a.createElement("br",null),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Deadline: ",l.a.createElement(ia.a,{date:a,renderer:Ca})),l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(Te.a,{container:!0},l.a.createElement(Te.a,{item:!0,xs:6,sm:4,lg:2},l.a.createElement(je.a,{variant:"h5",align:"left"},"Powered by:",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.cloudcv.org/web/challenges/challenge-page/573/"},l.a.createElement("img",{className:e.classes.bannerLogo+" "+e.classes.evalaiLogo,srcSet:"/assets/images/evalai_logo.png",alt:"EvalAI"})))))),l.a.createElement(Te.a,{item:!0,className:e.classes.overview,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"h4",align:"left"},"Overview"),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"TextCaps requires models to read and reason about text in images to generate captions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it and visual content in the image to generate image descriptions."),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"Current state-of-the-art models fail to generate captions for images in TextCaps because they do not have text reading and reasoning capabilities. See the examples in the image to compare ground truth answers and corresponding predictions by a state-of-the-art model."),l.a.createElement(je.a,{variant:"subtitle1",className:e.classes.headings,align:"left"},"Challenge link: ",l.a.createElement(K.a,{className:e.classes.evalAILink,target:"_blank",href:"https://evalai.cloudcv.org/web/challenges/challenge-page/573/"},"https://evalai.cloudcv.org/web/challenges/challenge-page/573/"))),l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12,md:4,lg:4},l.a.createElement("img",{className:e.classes.teaserImage,srcSet:"textcaps"===t?"/assets/images/textcaps_teaser.jpg":"/assets/images/teaser.png",alt:"Teaser"})),l.a.createElement(Te.a,{item:!0,style:{marginTop:0},className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Results and Analysis"),l.a.createElement(je.a,{variant:"subtitle1",gutterBottom:!0,align:"left"},l.a.createElement("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/9utGevwrSj8",frameBorder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0}))),l.a.createElement(Te.a,{item:!0,style:{marginTop:0},className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Starter Code")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Starter code is available in Pythia framework. Use ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/tree/project/m4c/"},"project/m4c")," branch to use TextCaps SoTA model, M4C. Find more details on how to use it at ",l.a.createElement(K.a,{href:"https://github.com/facebookresearch/pythia/tree/project/m4c/projects/M4C_Captioner/"},"this link")," and ",l.a.createElement(K.a,{href:"https://arxiv.org/abs/1911.06258"},"read the paper"),".")),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Prizes")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"We will be providing the winners of the first TextCaps 2020 challenge, the second TextVQA 2020 challenge and the first joint TextVQA and TextCaps 2020 challenge (coming soon) ",l.a.createElement(K.a,{href:"https://cloud.google.com/"},"Google Cloud Platform (GCP)"),"  credits worth $10k total. We thank GCP for their generosity.")),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Dates")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:8,lg:8},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"13 March 2019")," \u2014 Challenge announced."),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"15 May 2019 (23:59:59 GMT)")," \u2014 Submission deadline for participants."),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(je.a,{component:"span",className:e.classes.spanTypography,color:"primary"},"June 14th 2019")," \u2014 Winners' announcment at the ",l.a.createElement(K.a,{href:"https://visualqa.org/workshop.html"},"Visual Question Answering and Dialog Workshop, CVPR 2020"),"."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Winners will be invited to give a short talk at the workshop. ",l.a.createElement("br",null),"For questions about the challenge, join our ",l.a.createElement(K.a,{href:"https://groups.google.com/forum/#!forum/textvqa"},"Google Group")," or email us at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com"))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Dataset Description")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"You can find a detailed description and the download links for the dataset at the ",l.a.createElement(K.a,{href:"dataset"},"download")," page."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The challenge will be conducted on v0.1 of the ",l.a.createElement(K.a,{href:"/textcaps/dataset"},"TextCaps dataset"),", which is based on ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/web/index.html"},"OpenImages"),".",l.a.createElement("br",null),"TextCaps v0.1 contains 142,040 captions based on 28,408 images. The v0.1 training set contains 109,765 captions based on 21,953 images from OpenImages' training set. The v0.1 validation set contains 15,830 captions based on 3,166 images from OpenImages' training set while the v0.1 test-std set contains 16,445 captions based on 3,289 images from OpenImages' test set."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"To allow easier adoption, we also provide OCR tokens extracted using ",l.a.createElement(K.a,{href:"https://code.fb.com/ai-research/rosetta-understanding-text-in-images-and-videos-with-machine-learning/"},"Rosetta"),". Participants are free to use these OCR tokens and/or use other systems/ways to read/understand the text in the images.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Participation Guidelines")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Teams must register on ",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.cloudcv.org/"},"EvalAI")," and create a team for the challenge (",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.readthedocs.io/en/latest/participate.html"},"Quickstart"),")",l.a.createElement("br",null),"The challenge page is available at: ",l.a.createElement(K.a,{className:e.classes.evalAILink,href:"https://evalai.cloudcv.org/web/challenges/challenge-page/573/"},"https://evalai.cloudcv.org/web/challenges/challenge-page/573/")),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Challenge has three phases:")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,style:{width:"100%",overflowX:"auto"},xs:12,md:12,lg:12},l.a.createElement(oa.a,{style:{minWidth:700}},l.a.createElement(pa.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"No."),l.a.createElement(ga.a,null,l.a.createElement("b",null,"Phase")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Submissions")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Results")),l.a.createElement(ga.a,{align:"left"},l.a.createElement("b",null,"Leaderboard")))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#1"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"val")),l.a.createElement(ga.a,{align:"left"},"unlimited"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"none"))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#2"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"test-std")),l.a.createElement(ga.a,{align:"left"},"5 total combined with #3"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"public (optional)"))),l.a.createElement(ua.a,null,l.a.createElement(ba.a,null,l.a.createElement(ga.a,null,"#3"),l.a.createElement(ga.a,null,l.a.createElement("b",null,"test-violating-standard-guidelines")),l.a.createElement(ga.a,{align:"left"},"5 total combined with #2"),l.a.createElement(ga.a,{align:"left"},"immediate"),l.a.createElement(ga.a,{align:"left"},"none")))),l.a.createElement("br",null)),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"Please use the ",l.a.createElement("b",null,"validation")," split of TextCaps for the ",l.a.createElement("b",null,"val")," phase and the ",l.a.createElement("b",null,"test")," split for the ",l.a.createElement("b",null,"test-std")," phase. While answers are already provided for the ",l.a.createElement("b",null,"validation")," set, this phase is useful for sanity checking the result format without wasting submissions in the other phases. For the ",l.a.createElement("b",null,"test-std")," phase, the results must be submitted on the full set. Submissions to ",l.a.createElement("b",null,"test-std")," phase are considered entries into the challenge. By default, the submissions for the ",l.a.createElement("b",null,"test-std")," phase are private but can be voluntarily released to the public leaderboard, with a limit of one public leaderboard entry per team. At the end of the challenge, the entry with best accuracy from each team will be made public automatically and will be used for the challenge rankings. If you make an entry to ",l.a.createElement("b",null,"test-std")," and do not want to be considered for the challenge, please contact us at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com"),". We will contact the winning team to voluntarily present at the ",l.a.createElement(K.a,{href:"https://visualqa.org/workshop.html"},"Visual Question Answering and Dialog Workshop, CVPR 2020"),". Following guidelines must be followed for making a submission to ",l.a.createElement("b",null,"test-std"),".",l.a.createElement("b",null,"Note:")," Teams submitting to the challenge are required to submit a up-to 2 page abstract after the challenge. A detailed report on challenge analysis citing these abstracts will be provided later"),l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. Submission ",l.a.createElement("b",null,"should not use ensembles")," of any form for making predictions ",l.a.createElement("i",null,"i.e.")," it needs to be a single model.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Submission ",l.a.createElement("b",null,"should not use ground-truth OCRs")," or any kind of human-annotated text tokens for training or for making predictions. Note that submissions are allowed to use any kind of automatic OCR system. .")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. Total number of submissions from a participant team on ",l.a.createElement("b",null,"test-std")," and ",l.a.createElement("b",null,"test-violating-standard-guidelines")," (more details on this new phase below) combined should not exceed ",l.a.createElement("b",null,"5"),".")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Only one submission can be made public on ",l.a.createElement("b",null,"test-std")," leaderboard by one participant team."))),l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"Participants can submit to phase ",l.a.createElement("b",null,"test-violating-standard-guidelines")," for submissions that violate above guidelines. This phase doesn't have a leaderboard and submissions on this phase will remain private. The total number of submissions for ",l.a.createElement("b",null,"test-std")," and ",l.a.createElement("b",null,"test-violating-standard-guidelines")," combined should not exceed 5."),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"It is not acceptable to create multiple accounts for a single team in order to bypass the limits on number of submissions. A person can only be part of one particular team. The exception to this is if a group is working on multiple unrelated methods, in this case all sets of results can be submitted for evaluation. Results must be submitted to the evaluation server by the challenge deadline -- no exceptions will be made.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Abstract Submission Guidelines")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"All participants of the TextVQA and TextCaps Challenges can submit a short abstract (max 2 pages in CVPR camera ready format, references excluded) providing details of their submission. To be considered as challenge winner the submission of an abstract is required. The abstract must be submitted by ",l.a.createElement("b",null,"Wednesday, May 20th, 4pm PDT.")," Submit by email to textvqa@fb.com.",l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. The submission is NOT blind.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Please include all team members/authors names and their affiliation.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. Please include the team name used on the challenge server so we can correctly identify your results.")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Please let us know in the email if you are fine that your abstract PDF will be on the web page of the challenge."))),"Recommended content includes:",l.a.createElement(ge.b,{dense:!1},l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"1. List of all the data sources which were used for training")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"2. Experimental setup including training details, what pretraining was used (if any), how OCR tokens were incorporated")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"3. A table with ablations (on validation set)")),l.a.createElement(ge.c,null,l.a.createElement(ge.d,null,"4. Discussion on the insights you gained."))))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Submission Format")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"To submit to a phase, teams must upload a JSON file containing their model's answer prediction in the following format:"),l.a.createElement("br",null),l.a.createElement(je.a,{component:"span",align:"left",className:e.classes.preParent},l.a.createElement(je.a,{component:"pre"},l.a.createElement(je.a,{component:"span",variant:"body1"},l.a.createElement("code",null,JSON.stringify([{image_id:"STRING",caption:"STRING"},{image_id:"...",caption:"..."}],null,2))))),l.a.createElement(je.a,{className:e.classes.headings,variant:"subtitle1",align:"left"},"where ",l.a.createElement("b",null,"image_id")," is a image's unique id and ",l.a.createElement("b",null,"caption")," is the prediction by your model for the image. You can find an example submission file ",l.a.createElement(K.a,{href:"https://drive.google.com/file/d/1KpDGPUKILomUZY37b0N5urfMjF60eHNf/view?usp=sharing"},"here"),". When submitting, teams should also include a method name, method description, project URL, and publication URL if available.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{id:"evaluation",item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Evaluation")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"We report BLEU, METEOR, ROUGE-L, SPICE and CIDEr metrics for each submissions using ",l.a.createElement(K.a,{href:"https://github.com/tylin/coco-caption"},"coco-captions")," package. The leaderboard scores are sorted based on CIDEr and final ranking will be provided after the challenge based on a mixture of human evaluations and CIDEr.")),l.a.createElement(Te.a,{id:"evaluation",item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Related Datasets")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Following is a list of datasets related or similar to TextCaps. We encourage you to evaluate your approach on these as well, and/or consider using it as an additional source of training data. ",l.a.createElement("br",null),"1. ",l.a.createElement(K.a,{href:"/"},"TextVQA")," dataset is based on task of answering a question about an image. The questions require reading text in the image to be answered and are collected on the same images as TextVQA.")),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null)),l.a.createElement(Te.a,{item:!0,className:e.classes.headings,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"Organizers")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12,md:12,lg:12},l.a.createElement($e,{lgSize:3,people:fa[t][2020]})),l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignContent:"center",spacing:16,className:e.classes.sectionHeader},l.a.createElement(Te.a,{item:!0,xs:6,md:6,lg:4},l.a.createElement(K.a,{target:"_blank",href:"https://research.fb.com/category/facebook-ai-research/"},l.a.createElement("img",{className:e.classes.bannerLogo,style:{marginTop:"5%"},srcSet:"/assets/images/fair_logo.png",alt:"Facebook Artificial Intelligence Research"}))),l.a.createElement(Te.a,{item:!0,xs:3,md:3,lg:2},l.a.createElement(K.a,{target:"_blank",href:"https://www.gatech.edu/"},l.a.createElement("img",{className:e.classes.bannerLogo,srcSet:"/assets/images/gt_logo.png",alt:"Georgia Tech"}))),l.a.createElement(Te.a,{item:!0,xs:3,md:3,lg:2},l.a.createElement(K.a,{target:"_blank",href:"https://www.berkeley.edu/"},l.a.createElement("img",{className:e.classes.bannerLogo,style:{marginTop:"5%"},srcSet:"https://www.berkeley.edu/images/uploads/logo-ucberkeley.png",alt:"UC Berkeley"})))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement(Ye.a,null))),l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)))}),qa=t(65),ja=Object(g.withStyles)(function(e){return{title:{marginTop:"3em"},ulItems:{paddingLeft:"1.2em"},liLink:{borderBottom:"1px dotted #eee",borderBottomColor:e.palette.primary.light,paddingBottom:"0.1em","&:hover":{textDecoration:"none",borderBottomColor:e.palette.primary.main}},preParent:{backgroundColor:"#eee",border:"1px solid #ddd",borderRadius:"2px","& pre":{whiteSpace:"pre-wrap"},"& span":{padding:"1em"},"& code":{fontSize:"12px",color:"#000"}},downloadURLIcon:{verticalAlign:"middle",marginTop:"-0.1em"}}})(function(e){var a="https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5.1_train.json",t="https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5.1_val.json",n="https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5.1_test.json",s="https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip",r="https://dl.fbaipublicfiles.com/textvqa/images/test_images.zip",i="https://creativecommons.org/licenses/by/4.0/";return l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7},l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignItems:"center",spacing:16},l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12},l.a.createElement(je.a,{variant:"h3",align:"left"},"TextVQA dataset")),l.a.createElement("br",null),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"v0.5.1")),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"This version has everything same as ",l.a.createElement(K.a,{href:"/datasets#v0.5"},"v0.5")," except that Rosetta OCR tokens have been updated and separated into a separate JSON file.")),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Training set ",l.a.createElement(K.a,{href:a},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:a},"34,602 questions")," (103MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:s},"21,953 images")," (6.6GB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:"https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_Rosetta_OCR_v0.2_train.json"},"Rosetta OCR tokens [v0.2]"))))),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Validation set ",l.a.createElement(K.a,{href:t},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:t},"5,000 questions")," (16MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"3,166 images")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:"https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_Rosetta_OCR_v0.2_val.json"},"Rosetta OCR tokens [v0.2]"))))),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Test set ",l.a.createElement(K.a,{href:n},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:n},"5,734 questions")," (13MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:r},"3,289 images")," (926MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:"https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_Rosetta_OCR_v0.2_test.json"},"Rosetta OCR tokens [v0.2]"))))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"Challenge"),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"TextVQA Challenge 2020 is live! See more details on ",l.a.createElement(K.a,{href:"/challenge"},"challenge page")," to participate.")),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"Readme"),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Images for training and validation set are from OpenImages train set while images for test set are from OpenImages test set.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Validation set's images are contained in the zip for training set's images. The OpenImages dataset can be downloaded from ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/web/download.html"},"here"),".")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement("b",null,"Note:")," Some of the images in OpenImages are rotated, please make sure to check the ",l.a.createElement("b",null,"Rotation")," field in the Image IDs files for ",l.a.createElement(K.a,{target:"_blank",href:"https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv"},"train")," and ",l.a.createElement(K.a,{target:"_blank",href:"https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv"},"test"),".")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Data is available under ",l.a.createElement(K.a,{href:i},"CC BY 4.0")," license.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"TextVQA evaluation server for testing and validation set is hosted on ",l.a.createElement(K.a,{target:"_blank",href:"https://evalai.cloudcv.org/web/challenges/challenge-page/244/"},"EvalAI"),".")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Numbers in the papers should be reported on v0.5.1/v0.5 test set (test-std).",l.a.createElement("b",null,"NOTE:")," Both v0.5.1 and v0.5 are same except the OCR tokens. See top.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"We also provide OCR tokens extracted from ",l.a.createElement(K.a,{target:"_blank",href:"https://code.fb.com/ai-research/rosetta-understanding-text-in-images-and-videos-with-machine-learning/"},"Rosetta")," system with the dataset.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement("b",null,"OCR tokens")," provided in the dataset better than the ones used in the TextVQA ",l.a.createElement(K.a,{href:"paper"},"paper"),", but are nowhere near perfect. Original OCR tokens that were used in the original paper can be found in the data for v0.5 provided below. Researchers are welcome to use their own OCR systems.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Reach us out at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com")," for any questions, suggestions and feedback.")))),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"Description"),l.a.createElement("br",null),l.a.createElement(je.a,{align:"left"},"TextVQA JSON files"),l.a.createElement(je.a,{component:"span",align:"left",className:e.classes.preParent},l.a.createElement(je.a,{component:"pre"},l.a.createElement(je.a,{component:"span",variant:"body1"},l.a.createElement("code",null,JSON.stringify(qa.textvqa,null,2))))),l.a.createElement("br",null),l.a.createElement(je.a,{align:"left"},"OCR JSON files"),l.a.createElement(je.a,{component:"span",align:"left",className:e.classes.preParent},l.a.createElement(je.a,{component:"pre"},l.a.createElement(je.a,{component:"span",variant:"body1"},l.a.createElement("code",null,JSON.stringify(qa.ocr,null,2)))))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)),l.a.createElement(Te.a,{id:"v0.5",item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"v0.5")),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Training set ",l.a.createElement(K.a,{href:a},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:a.replace("0.5.1","0.5")},"34,602 questions")," (103MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:s},"21,953 images")," (6.6GB)")))),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Validation set ",l.a.createElement(K.a,{href:t},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:t.replace("0.5.1","0.5")},"5,000 questions")," (16MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"3,166 images")))),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Test set ",l.a.createElement(K.a,{href:n},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:n.replace("0.5.1","0.5")},"5,734 questions")," (13MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:r},"3,289 images")," (926MB)")))),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"License"),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{href:i},"CC BY 4.0")))),l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)))}),La=Object(g.withStyles)(function(e){return{embed:{marginTop:"4em",width:"100%",height:"90vh"},buttonLink:{color:e.palette.primary.primaryText,textDecoration:"none"},buttonsSide:{textAlign:"right"},leftIcon:{marginRight:e.spacing.unit}}})(function(e){return"textocr"===Z()?"Code will be available soon!":l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:12,md:10,lg:8,style:{marginTop:"4em"}},"Code is available at ",l.a.createElement(K.a,{underline:"none",className:[e.classes.buttonLink,e.classes.buttonsSide].join(" "),href:"https://github.com/facebookresearch/pythia"},"https://github.com/facebookresearch/pythia"),".",l.a.createElement("br",null),l.a.createElement("br",null),"Pythia is a modular framework for multimodal (vision + language) research and can be used as a starter point for working on TextVQA."))}),Ma=Object(g.withStyles)(function(e){return{embed:{marginTop:"4em",width:"100%",height:"90vh"},buttonLink:{color:e.palette.primary.contrastText,textDecoration:"none"},buttonsSide:{textAlign:"right"},leftIcon:{marginRight:e.spacing.unit}}})(function(e){var a=Z(),t="textcaps"===a?"https://dl.fbaipublicfiles.com/textvqa/data/textcaps/textcaps.pdf":"https://arxiv.org/abs/1904.08920",n="textcaps"===a?"https://dl.fbaipublicfiles.com/textvqa/data/textcaps/textcaps.pdf":"https://arxiv.org/pdf/1904.08920";return"textocr"===a?"Manuscript will be available soon!":l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:12,md:10,lg:8,style:{marginTop:"4em"}},l.a.createElement(K.a,{underline:"none",className:[e.classes.buttonLink,e.classes.buttonsSide].join(" "),href:t},l.a.createElement(y.a,{variant:"contained",color:"primary"},l.a.createElement(j.a,{className:e.classes.leftIcon}),"Download Paper")),l.a.createElement("embed",{className:e.classes.embed,src:n,type:"application/pdf"})))}),Va=Object(g.withStyles)(function(e){return{title:{marginTop:"3em"},ulItems:{paddingLeft:"1.2em"},liLink:{borderBottom:"1px dotted #eee",borderBottomColor:e.palette.primary.light,paddingBottom:"0.1em","&:hover":{textDecoration:"none",borderBottomColor:e.palette.primary.main}},preParent:{backgroundColor:"#eee",border:"1px solid #ddd",borderRadius:"2px","& pre":{whiteSpace:"pre-wrap"},"& span":{padding:"1em"},"& code":{fontSize:"12px",color:"#000"}},downloadURLIcon:{verticalAlign:"middle",marginTop:"-0.1em"}}})(function(e){var a="https://dl.fbaipublicfiles.com/textvqa/data/textcaps/TextCaps_0.1_train.json",t="https://dl.fbaipublicfiles.com/textvqa/data/textcaps/TextCaps_0.1_val.json",n="https://dl.fbaipublicfiles.com/textvqa/data/textcaps/TextCaps_0.1_test.json",s="https://creativecommons.org/licenses/by/4.0/";return l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7},l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignItems:"center",spacing:16},l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12},l.a.createElement(je.a,{variant:"h3",align:"left"},"TextCaps dataset")),l.a.createElement("br",null),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"v0.1")),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Training set ",l.a.createElement(K.a,{href:a},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:a},"109,765 captions")," (173MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:"https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip"},"21,953 images")," (6.6GB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:"https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_Rosetta_OCR_v0.2_train.json"},"Rosetta OCR tokens [v0.2]"))))),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Validation set ",l.a.createElement(K.a,{href:t},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:t},"15,830 captions")," (25MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"3,166 images")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:"https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_Rosetta_OCR_v0.2_val.json"},"Rosetta OCR tokens [v0.2]"))))),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Test set ",l.a.createElement(K.a,{href:n},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:n},"Metadata")," (6.5MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:"https://dl.fbaipublicfiles.com/textvqa/images/test_images.zip"},"3,289 images")," (926MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:"https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_Rosetta_OCR_v0.2_test.json"},"Rosetta OCR tokens [v0.2]"))))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"Challenge"),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"TextCaps Challenge 2020 is live! See more details on ",l.a.createElement(K.a,{href:"/textcaps/challenge"},"challenge page")," to participate.")),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"Readme"),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Images for training and validation set are from OpenImages train set while images for test set are from OpenImages test set.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Validation set's images are contained in the zip for training set's images. The OpenImages dataset can be downloaded from ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/web/download.html"},"here"),".")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement("b",null,"Note:")," Some of the images in OpenImages are rotated, please make sure to check the ",l.a.createElement("b",null,"Rotation")," field in the Image IDs files for ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv"},"train")," and ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv"},"test"),".")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Data is available under ",l.a.createElement(K.a,{href:s},"CC BY 4.0")," license.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"TextCaps evaluation server for testing and validation set is hosted on ",l.a.createElement(K.a,{href:"https://evalai.cloudcv.org/web/challenges/challenge-page/573/"},"EvalAI"),".")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Numbers in the papers should be reported on v0.1 test set (test-std).")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"We also provide OCR tokens extracted from ",l.a.createElement(K.a,{href:"https://code.fb.com/ai-research/rosetta-understanding-text-in-images-and-videos-with-machine-learning/"},"Rosetta")," system with the dataset.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Reach us out at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com")," for any questions, suggestions and feedback.")))),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"Description"),l.a.createElement("br",null),l.a.createElement(je.a,{align:"left"},"TextCaps JSON files"),l.a.createElement(je.a,{component:"span",align:"left",className:e.classes.preParent},l.a.createElement(je.a,{component:"pre"},l.a.createElement(je.a,{component:"span",variant:"body1"},l.a.createElement("code",null,JSON.stringify(qa.textcaps,null,2))))),l.a.createElement("br",null),l.a.createElement(je.a,{align:"left"},"OCR JSON files"),l.a.createElement(je.a,{component:"span",align:"left",className:e.classes.preParent},l.a.createElement(je.a,{component:"pre"},l.a.createElement(je.a,{component:"span",variant:"body1"},l.a.createElement("code",null,JSON.stringify(qa.ocr,null,2)))))),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"License"),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{href:s},"CC BY 4.0")))),l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)))}),Pa=t(70),Da=function(e){return"textcaps"===(-1===Object(p.e)().pathname.indexOf("textcaps")?"textvqa":"textcaps")?l.a.createElement("div",null,l.a.createElement(Pa.Helmet,null,l.a.createElement("meta",{charset:"utf-8"}))):l.a.createElement("div",null,l.a.createElement(Pa.Helmet,null,l.a.createElement("title",null,"TextVQA"),l.a.createElement("meta",{charset:"utf-8"}),l.a.createElement("link",{rel:"shortcut icon",href:"/assets/images/textvqa_favicon.png?v=2"}),l.a.createElement("meta",{name:"viewport",content:"width=device-width, initial-scale=1, shrink-to-fit=no"}),l.a.createElement("meta",{name:"theme-color",content:"#000000"}),l.a.createElement("meta",{property:"og:title",content:"TextVQA"}),l.a.createElement("meta",{property:"og:description",content:"A dataset to benchmark visual reasoning based on text in images."}),l.a.createElement("meta",{property:"og:image",content:"https://textvqa.org/assets/images/textvqa_teaser.png"}),l.a.createElement("meta",{property:"og:url",content:"https://textvqa.org"}),l.a.createElement("meta",{property:"og:site_name",content:"TextVQA"}),l.a.createElement("meta",{name:"twitter:title",content:"TextVQA"}),l.a.createElement("meta",{name:"twitter:description",content:"A dataset to benchmark visual reasoning based on text in images."}),l.a.createElement("meta",{name:"twitter:image",content:"https://textvqa.org/assets/images/textvqa_teaser.png"}),l.a.createElement("meta",{name:"twitter:card",content:"summary_large_image"}),l.a.createElement("meta",{name:"twitter:image:alt",content:"A dataset to benchmark visual reasoning based on text in images."})))},Fa=function(e){return l.a.createElement("div",null,l.a.createElement(Pa.Helmet,null,l.a.createElement("title",null,"TextCaps"),l.a.createElement("meta",{charset:"utf-8"}),l.a.createElement("link",{rel:"shortcut icon",href:"/assets/images/textcaps_favicon.png?v=2"}),l.a.createElement("meta",{name:"viewport",content:"width=device-width, initial-scale=1, shrink-to-fit=no"}),l.a.createElement("meta",{name:"theme-color",content:"#000000"}),l.a.createElement("meta",{property:"og:title",content:"TextCaps"}),l.a.createElement("meta",{property:"og:description",content:"A dataset to benchmark image captioning based on text in images."}),l.a.createElement("meta",{property:"og:image",content:"https://textvqa.org/assets/images/textcaps_logo_and_text.png"}),l.a.createElement("meta",{property:"og:url",content:"https://textvqa.org/textcaps"}),l.a.createElement("meta",{property:"og:site_name",content:"TextCaps"}),l.a.createElement("meta",{name:"twitter:title",content:"TextCaps"}),l.a.createElement("meta",{name:"twitter:description",content:"A dataset to benchmark image captioning based on text in images."}),l.a.createElement("meta",{name:"twitter:image",content:"https://textvqa.org/assets/images/textcaps_logo_and_text.png"}),l.a.createElement("meta",{name:"twitter:card",content:"summary_large_image"}),l.a.createElement("meta",{name:"twitter:image:alt",content:"A dataset to benchmark image captioning based on text in images."})))},Qa=function(e){return l.a.createElement("div",null,l.a.createElement(Pa.Helmet,null,l.a.createElement("title",null,"TextOCR"),l.a.createElement("meta",{charset:"utf-8"}),l.a.createElement("link",{rel:"shortcut icon",href:"/assets/images/textocr/logo_only.png?v=2"}),l.a.createElement("meta",{name:"viewport",content:"width=device-width, initial-scale=1, shrink-to-fit=no"}),l.a.createElement("meta",{name:"theme-color",content:"#000000"}),l.a.createElement("meta",{property:"og:title",content:"TextOCR"}),l.a.createElement("meta",{property:"og:description",content:"A dataset for arbitrary shaped text recognition on natural images"}),l.a.createElement("meta",{property:"og:image",content:"https://textvqa.org/assets/images/textocr/logo_horizontal_color_with_text.png"}),l.a.createElement("meta",{property:"og:url",content:"https://textvqa.org/textocr"}),l.a.createElement("meta",{property:"og:site_name",content:"TextOCR"}),l.a.createElement("meta",{name:"twitter:title",content:"TextOCR"}),l.a.createElement("meta",{name:"twitter:description",content:"A dataset for arbitrary shaped text recognition on natural images"}),l.a.createElement("meta",{name:"twitter:image",content:"https://textvqa.org/assets/images/textocr/logo_horizontal_color_with_text.png"}),l.a.createElement("meta",{name:"twitter:card",content:"summary_large_image"}),l.a.createElement("meta",{name:"twitter:image:alt",content:"A dataset for arbitrary shaped text recognition on natural images"})))},Ba=Object(g.withStyles)(function(e){return{title:{marginTop:"3em"},ulItems:{paddingLeft:"1.2em"},liLink:{borderBottom:"1px dotted #eee",borderBottomColor:e.palette.primary.light,paddingBottom:"0.1em","&:hover":{textDecoration:"none",borderBottomColor:e.palette.primary.main}},preParent:{backgroundColor:"#eee",border:"1px solid #ddd",borderRadius:"2px","& pre":{whiteSpace:"pre-wrap"},"& span":{padding:"1em"},"& code":{fontSize:"12px",color:"#000"}},downloadURLIcon:{verticalAlign:"middle",marginTop:"-0.1em"}}})(function(e){var a="https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_train.json",t="https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_val.json",n="https://dl.fbaipublicfiles.com/textvqa/data/textocr/TextOCR_0.1_test.json",s="https://creativecommons.org/licenses/by/4.0/";return l.a.createElement(Te.a,{container:!0,justify:"center",alignItems:"center"},l.a.createElement(Te.a,{item:!0,xs:10,md:8,lg:7},l.a.createElement(Te.a,{container:!0,justify:"flex-start",alignItems:"center",spacing:16},l.a.createElement(Te.a,{item:!0,className:e.classes.title,xs:12},l.a.createElement(je.a,{variant:"h3",align:"left"},"TextOCR dataset")),l.a.createElement("br",null),l.a.createElement(Te.a,{item:!0,className:e.classes.versionNumber,xs:12},l.a.createElement(je.a,{variant:"h4",align:"left"},"v0.1")),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Training set ",l.a.createElement(K.a,{href:a},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:a},"714,770 word annotations")," (272MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:"https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip"},"21,778 images")," (6.6GB)")))),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Validation set ",l.a.createElement(K.a,{href:t},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:t},"107,802 word annotations")," (39MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"3,124 images")))),l.a.createElement(Te.a,{className:e.classes.setItems,item:!0,xs:12,sm:6,md:4},l.a.createElement(je.a,{variant:"h5",align:"left"},"Test set ",l.a.createElement(K.a,{href:n},l.a.createElement(j.a,{className:e.classes.downloadURLIcon}))),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:n},"Metadata")," (1MB)")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{className:e.classes.liLink,href:"https://dl.fbaipublicfiles.com/textvqa/images/test_images.zip"},"3,232 images")," (926MB)")))),l.a.createElement(Te.a,{item:!0,xs:12},l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"Challenge"),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"We will be soon hosting a challenge on TextOCR test set. Reach us out at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com")," for any questions.")),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"Readme"),l.a.createElement(je.a,{variant:"subheading",align:"left"},l.a.createElement("b",null,"General Information")),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Data is available under ",l.a.createElement(K.a,{href:s},"CC BY 4.0")," license.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Numbers in the papers should be reported on v0.1 test set. We will soon host a challenge on that.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Reach us out at ",l.a.createElement(K.a,{href:"mailto:textvqa@fb.com"},"textvqa@fb.com")," for any questions, suggestions and feedback."))),l.a.createElement(je.a,{variant:"subheading",align:"left"},l.a.createElement("b",null,"Images")),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Images for training and validation set are from OpenImages train set while images for test set are from OpenImages test set.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Validation set's images are contained in the zip for training set's images. The OpenImages dataset can be downloaded from ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/web/download.html"},"here"),".")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement("b",null,"Note:")," Some of the images in OpenImages are rotated, please make sure to check the ",l.a.createElement("b",null,"Rotation")," field in the Image IDs files for ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv"},"train")," and ",l.a.createElement(K.a,{href:"https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv"},"test"),"."))),l.a.createElement(je.a,{variant:"subheading",align:"left"},l.a.createElement("b",null,"Dataset Format")),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},'The json format mostly follows COCO-Text v2, except the "mask" field in "anns" is named as "points" for the polygon annotation.')),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},'The "points" field is a list of 2D coordinates like [x1, y1, x2, y2, ...]. Note that (x1,y1) is always the top-left corner of the text (in its own orientation), and order of the points is clockwise (for example, for horizontal text, (x1, y1) and (x2, y2) will form the top line).')),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},'The "bbox" field contains horizontal box converted from "points" for convenience, and "area" is computed based on width and height of "bbox". For any conversion to other formats such as rotated boxes or quadrilaterals, "points" should be used as the source of truth.'))),l.a.createElement(je.a,{variant:"subheading",align:"left"},l.a.createElement("b",null,"Annotation Details")),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},'In cases when the text is illegible or not in English, polygon is annotated normally but word will be annotated as a single "." symbol.')),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"Word annotations are case-sensitive, and can contain punctuations too.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The annotators were instructed to draw exactly 4 points (quadrilaterals) whenever possible, and only draw more than 4 points when necessary (for cases like curved text)."))),l.a.createElement(je.a,{variant:"subheading",align:"left"},l.a.createElement("b",null,"Relationship with TextVQA/TextCaps")),l.a.createElement("ul",{className:e.classes.ulItems},l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"The image ids in TextOCR match the ids in TextVQA.")),l.a.createElement("li",null,l.a.createElement(je.a,{variant:"subtitle1",align:"left"},"train/val/test splits are the same as TextVQA/TextCaps. However due to privacy reasons, we removed 274 images from TextVQA while creating TextOCR.")))),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"Description"),l.a.createElement("br",null),l.a.createElement(je.a,{align:"left"},"TextOCR JSON files"),l.a.createElement(je.a,{component:"span",align:"left",className:e.classes.preParent},l.a.createElement(je.a,{component:"pre"},l.a.createElement(je.a,{component:"span",variant:"body1"},l.a.createElement("code",null,JSON.stringify(qa.textocr,null,2))))),l.a.createElement("br",null)),l.a.createElement(Te.a,{item:!0,xs:12,md:12},l.a.createElement(je.a,{variant:"h6",align:"left"},"License"),l.a.createElement("br",null),l.a.createElement(je.a,{variant:"subtitle1",align:"left"},l.a.createElement(K.a,{href:s},"CC BY 4.0")))),l.a.createElement("br",null),l.a.createElement(Ye.a,null),l.a.createElement("br",null)))}),za=function(e){function a(){return Object(c.a)(this,a),Object(m.a)(this,Object(u.a)(a).apply(this,arguments))}return Object(h.a)(a,e),Object(o.a)(a,[{key:"render",value:function(){return l.a.createElement(g.MuiThemeProvider,{theme:ae},l.a.createElement(d.a,{basename:"/"},l.a.createElement("div",{className:"App"},l.a.createElement("header",{className:"App-header"},l.a.createElement(ve,null)),l.a.createElement("div",null,l.a.createElement(p.a,{path:"/",exact:!0,component:ra}),l.a.createElement(p.a,{path:"/",component:Da}),l.a.createElement(p.a,{path:"/textcaps",component:Fa}),l.a.createElement(p.a,{path:"/textcaps",exact:!0,component:ra}),l.a.createElement(p.a,{path:"/textocr",component:Qa}),l.a.createElement(p.a,{path:"/textocr",exact:!0,component:ra}),l.a.createElement(p.a,{path:"/:type?/explore",component:We}),l.a.createElement(p.a,{exact:!0,path:"/textcaps/challenge",component:Aa}),l.a.createElement(p.a,{exact:!0,path:"/textcaps/challenge/2021",component:Aa}),l.a.createElement(p.a,{exact:!0,path:"/textcaps/challenge/2020",component:Sa}),l.a.createElement(p.a,{exact:!0,path:"/challenge/2019",component:Na}),l.a.createElement(p.a,{exact:!0,path:"/challenge/2020",component:Oa}),l.a.createElement(p.a,{exact:!0,path:"/challenge",component:Ia}),l.a.createElement(p.a,{exact:!0,path:"/challenge/2021",component:Ia}),l.a.createElement(p.a,{exact:!0,path:"/download",component:ja}),l.a.createElement(p.a,{exact:!0,path:"/textcaps/download",component:Va}),l.a.createElement(p.a,{exact:!0,path:"/textocr/download",component:Ba}),l.a.createElement(p.a,{path:"/:type?/code",component:La}),l.a.createElement(p.a,{exact:!0,path:"/dataset",component:ja}),l.a.createElement(p.a,{exact:!0,path:"/textcaps/dataset",component:Va}),l.a.createElement(p.a,{exact:!0,path:"/textocr/dataset",component:Ba}),l.a.createElement(p.a,{path:"/:type?/paper",component:Ma})))))}}]),a}(n.Component);Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));i.init({dsn:Ie.b}),r.a.render(l.a.createElement(za,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then(function(e){e.unregister()})},65:function(e){e.exports={textvqa:{data:[{question_id:"INT, incremental unique ID for the question",question:"what is ....?",question_tokens:["token_1","token_2","...","token_N"],image_id:"OpenImages Image ID",image_classes:["OpenImages_class_1","OpenImages_class_2","...","OpenImages_class_n"],flickr_original_url:"OpenImages original flickr url",flickr_300k_url:"OpenImages flickr 300k thumbnail url",image_width:"INT, Width of the image",image_height:"INT, Height of the image",set_name:"Dataset split question belongs to",answers:["answer_1","answer_2","...","answer_10"]}],dataset_type:"Split train|val|test",dataset_name:"textvqa",dataset_version:"VERSION"},textcaps:{data:[{image_id:"Alphanumeric String, unique OpenImages Image ID",caption_str:"The poster says ...",caption_tokens:["token_1","token_2","...","token_N"],caption_id:"INT, incremental unique caption ID",image_classes:["OpenImages_class_1","OpenImages_class_2","...","OpenImages_class_n"],flickr_original_url:"OpenImages original flickr url",flickr_300k_url:"OpenImages flickr 300k thumbnail url",image_width:"INT, Width of the image",image_height:"INT, Height of the image",set_name:"Dataset split question belongs to",reference_strs:["caption_1","...","...","caption_5"],reference_tokens:[["token_1_1","...","token_1_n"],["..."],["token_5_1","...","token_5_n"]]}],dataset_type:"Split train|val|test",dataset_name:"textcaps",dataset_version:"VERSION"},ocr:{data:{image_id:"STRING",ocr_tokens:["OCR extracted text 1","OCR extracted text 2","...","OCR extracted text N"],ocr_info:[{bounding_box:{top_left_x:"FLOAT, top left x coordinate of the OCR box",top_left_y:"FLOAT, top left y coordinate of the OCR box",width:"FLOAT (between 0 to 1), relative width of the OCR box",height:"FLOAT (between 0 to 1), relative height of the OCR box",rotation:"Rotation angle of the extracted text",yaw:"Yaw of the extracted text",roll:"Roll of the extracted text",pitch:"Pitch of the extracted text"},word:"Extracted word unformatted"}]},dataset_type:"Split train|val|test",dataset_name:"textvqa_ocr",dataset_version:"VERSION"},textocr:{imgs:{OpenImages_ImageID_1:{id:"OpenImages_ImageID_1",width:"INT, Width of the image",height:"INT, Height of the image",set:"Split train|val|test",filename:"train|test/OpenImages_ImageID_1.jpg"},OpenImages_ImageID_2:{"...":"..."}},anns:{OpenImages_ImageID_1_1:{id:"STR, OpenImages_ImageID_1_1, Specifies the nth annotation for an image",image_id:"OpenImages_ImageID_1",bbox:["FLOAT x1","FLOAT y1","FLOAT x2","FLOAT y2"],points:["FLOAT x1","FLOAT y1","FLOAT x2","FLOAT y2","...","FLOAT xN","FLOAT yN"],utf8_string:"text for this annotation",area:"FLOAT, area of this box"},OpenImages_ImageID_1_2:{"...":"..."},OpenImages_ImageID_2_1:{"...":"..."}},img2Anns:{OpenImages_ImageID_1:["OpenImages_ImageID_1_1","OpenImages_ImageID_1_2","OpenImages_ImageID_1_2"],OpenImages_ImageID_N:["..."]}}}},86:function(e){e.exports={textvqa:{news:[{date:"Mar 2020",news:"<a href='/textcaps/challenge'>TextCaps Challenge 2021</a> announced on the <a href='/textcaps/download/'>TextCaps v0.1 dataset</a>."},{date:"Mar 2021",news:"<a href='/challenge/'>TextVQA Challenge 2021</a> announced on the <a href='/download'>TextVQA v0.5.1 dataset</a>."},{date:"Jul 2020",news:"<a href='/textcaps'>TextCaps</a> paper has been accepted at ECCV 2020 as an <a href='https://www.youtube.com/watch?v=bWOnRpqmom4'>oral presentation</a>!"},{date:"Jun 2020",news:"<a href='/challenge/2020'>TextVQA Challenge 2020</a> winners announced. Checkout analysis and results <a href='https://www.youtube.com/watch?v=O5y8i3OYdo8'>video</a>."},{date:"Mar 2020",news:"<a href='/textcaps/challenge/2020'>TextCaps Challenge 2020</a> announced on the <a href='/textcaps/download/'>TextCaps v0.1 dataset</a>."},{date:"Mar 2020",news:"<a href='/challenge/2020'>TextVQA Challenge 2020</a> announced on the <a href='/download'>TextVQA v0.5.1 dataset</a>."},{date:"Mar 2020",news:"<a href='/download'>TextVQA v0.5.1</a> is now public with new updated OCR tokens."},{date:"Jan 2020",news:"<a href='https://arxiv.org/abs/1911.06258'>M4C</a>, new TextVQA SoTA model's is now available in <a href-'https://github.com/facebookresearch/pythia/tree/project/m4c/projects/M4C'>Pythia</a>."},{date:"May 2019",news:"Deadline for <a href='challenge/2019'>TextVQA challenge</a> extended to May 27th 2019."},{date:"Apr 2019",news:"Starter code for <a href='challenge/2019'>TextVQA challenge</a> and LoRRA are now available in <a href='https://github.com/facebookresearch/pythia'>Pythia's new version</a>!"},{date:"Apr 2019",news:"TextVQA paper 'Towards VQA Models That Can Read' is available on <a href='https://arxiv.org/abs/1904.08920'>arXiV</a>!"},{date:"Mar 2019",news:"<a href='challenge/2019'>TextVQA Challenge 2019</a> announced on the <a href='download'>TextVQA 0.5 dataset</a>!"},{date:"Mar 2019",news:"<a href='download'>TextVQA v0.5</a> containing 45,336 questions released."}]},textcaps:{news:[{date:"Mar 2021",news:"<a href='/textcaps/challenge'>TextCaps Challenge 2021</a> announced on the <a href='/textcaps/download/'>TextCaps v0.1 dataset</a>."},{date:"Jul 2020",news:"<a href='/textcaps'>TextCaps</a> paper has been accepted at ECCV 2020 as an <a href='https://www.youtube.com/watch?v=bWOnRpqmom4'>oral presentation</a>!"},{date:"Jun 2020",news:"<a href='/textcaps/challenge/2020'>TextCaps Challenge 2020</a> winners announced. Checkout analysis and results <a href='https://www.youtube.com/watch?v=9utGevwrSj8'>video</a>."},{date:"Mar 2020",news:"<a href='/textcaps/challenge/2020'>TextCaps Challenge 2020</a> announced on the <a href='/textcaps/download/'>TextCaps v0.1 dataset</a>."},{date:"Mar 2020",news:"<a href='/textcaps/download'>TextCaps v0.1</a> containing 142,040 captions released."}]},textocr:{news:[{date:"May 2021",news:"<a href='/textocr/download'>TextOCR v0.1</a> containing 903,069 word annotations released."}]}}}},[[319,1,2]]]);
//# sourceMappingURL=main.8941bec8.chunk.js.map